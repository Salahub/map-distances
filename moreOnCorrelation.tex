\documentclass{article}

% For PDF, suitable for double-sided printing, change the PrintVersion variable below to "true" and use this \documentclass line instead of the one above:
%\documentclass[letterpaper,12pt,titlepage,openright,twoside,final]{book}
\newcommand{\package}[1]{\textbf{#1}} % package names in bold text
\newcommand{\cmmd}[1]{\textbackslash\texttt{#1}} % command name in tt font 
\newcommand{\href}[1]{#1} % does nothing, but defines the command so the print-optimized version will ignore \href tags (redefined by hyperref pkg).
%\newcommand{\texorpdfstring}[2]{#1} % does nothing, but defines the command
% Anything defined here may be redefined by packages added below...

% This package allows if-then-else control structures.
\usepackage{ifthen}
\newboolean{PrintVersion}
\setboolean{PrintVersion}{false}
% CHANGE THIS VALUE TO "true" as necessary, to improve printed results for hard copies by overriding some options of the hyperref package, called below.

%\usepackage{nomencl} % For a nomenclature (optional; available from ctan.org)
\usepackage{amsmath,amssymb,amstext} % Lots of math symbols and environments
\usepackage[pdftex]{graphicx} % For including graphics N.B. pdftex graphics driver
\usepackage{amsmath,amssymb,amstext,amsthm,amsfonts}
\usepackage{dsfont}
\usepackage[pdftex]{graphicx}
\usepackage{caption}
\usepackage{color}% Include colors for document elements
\usepackage{dcolumn}% Align table columns on decimal point
\usepackage{bm}% bold math
\usepackage{float}
\usepackage{multirow}
\usepackage[round]{natbib}   % omit 'round' option for square brackets

\usepackage{algorithm} % For counting chapters
\usepackage{algorithmicx, algpseudocode}
%\renewcommand{\algorithmiccomment}[1]{// #1} % Brackets are confused with the sets
%\algsetup{linenosize=\scriptsize}

% N.B. HYPERREF MUST BE THE LAST PACKAGE LOADED; ADD ADDITIONAL PKGS ABOVE
\usepackage[pdftex,pagebackref=false]{hyperref} % with basic options
%\usepackage[pdftex,pagebackref=true]{hyperref}
% N.B. pagebackref=true provides links back from the References to the body text. This can cause trouble for printing.
% define colours
\definecolor{background-color}{gray}{0.98}
\definecolor{steelblue}{rgb}{0.27, 0.51, 0.71}
\definecolor{brickred}{rgb}{0.8, 0.25, 0.33}
\definecolor{bluegray}{rgb}{0.4, 0.6, 0.8}
\definecolor{amethyst}{rgb}{0.6, 0.4, 0.8}

\hypersetup{
	plainpages=false,       % needed if Roman numbers in frontpages
	unicode=false,          % non-Latin characters in Acrobat's bookmarks
	pdftoolbar=true,        % show Acrobats toolbar?
	pdfmenubar=true,        % show Acrobat's menu?
	pdffitwindow=false,     % window fit to page when opened
	pdfstartview={FitH},    % fits the width of the page to the window
	pdftitle={Genetic correlation},    % title: CHANGE THIS TEXT!
	pdfauthor={Chris Salahub},    % author: CHANGE THIS TEXT! and uncomment this line
	%pdfsubject={Statistics},  % subject: CHANGE THIS TEXT! and uncomment this line
	%    pdfkeywords={keyword1} {key2} {key3}, % list of keywords, and uncomment this line if desired
	pdfnewwindow=true,      % links in new window
	colorlinks=true,        % false: boxed links; true: colored links
	linkcolor=steelblue,         % color of internal links
	citecolor=brickred,        % color of links to bibliography
	filecolor=magenta,      % color of file links
	urlcolor=cyan           % color of external links
}
\ifthenelse{\boolean{PrintVersion}}{   % for improved print quality, change some hyperref options
	\hypersetup{	% override some previously defined hyperref options
		%    colorlinks,%
		citecolor=black,%
		filecolor=black,%
		linkcolor=black,%
		urlcolor=black}
}{} % end of ifthenelse (no else)

%\usepackage[automake,toc,abbreviations]{glossaries-extra} % Exception to the rule of hyperref being the last add-on package

% Page margins
% uWaterloo thesis requirements specify a minimum of 1 inch (72pt) margin at the
% top, bottom, and outside page edges and a 1.125 in. (81pt) gutter margin (on binding side). 
\setlength{\marginparwidth}{0pt} % width of margin notes
% N.B. If margin notes are used, you must adjust \textwidth, \marginparwidth
% and \marginparsep so that the space left between the margin notes and page
% edge is less than 15 mm (0.6 in.)
\setlength{\marginparsep}{0pt} % width of space between body text and margin notes
\setlength{\evensidemargin}{0.125in} % Adds 1/8 in. to binding side of all even pages when "twoside" is selected
\setlength{\oddsidemargin}{0.125in} % Adds 1/8 in. to the left of all pages when "oneside" is selected,
% and to the left of all odd pages when "twoside" is selected
\setlength{\textwidth}{6.375in} % assuming US letter paper (8.5 in. x 11 in.) and margins as above
\raggedbottom

\setlength{\parskip}{\medskipamount} % space between paragraphs
\renewcommand{\baselinestretch}{1} % line space setting

% Commands
% Code
\newcommand{\code}[1]{\texttt{#1}}
\newcommand*{\Rnsp}{\textsf{R}}
\newcommand*{\R}{\textsf{R}$~$}
\newcommand*{\Pythonnsp}{\textsf{Python}}
\newcommand*{\Python}{\textsf{Python}$~$}
\newcommand{\pkg}[1]{\textsf{#1}}
\newcommand{\pkgsp}[1]{\textsf{#1}$~$}
\algblock{Input}{EndInput}
\algnotext{EndInput}
\newcommand{\Desc}[2]{\State \makebox[2em][l]{#1}#2}

% Theorem styles
\newtheorem{definition}{Definition}
\newtheorem{theorem}{Theorem}

% vectors
\newcommand{\ve}[1]{\mathbf{#1}}           % for vectors
\newcommand{\sv}[1]{\boldsymbol{#1}}   % for greek letters
\newcommand{\m}[1]{\mathbf{#1}}               % for matrices
\newcommand{\sm}[1]{\boldsymbol{#1}}   % for greek letters
\newcommand{\tr}[1]{{#1}^{\mkern-1.5mu\mathsf{T}}}              % for transpose
\newcommand{\conj}[1]{{#1}^{\ast}}
\newcommand{\norm}[1]{||{#1}||}              % norm
\newcommand{\frob}[1]{\norm{#1}_F}
\newcommand{\abs}[1]{\lvert{#1}\rvert}              % norm
\newcommand*{\mvec}{\operatorname{vec}}
\newcommand*{\trace}{\operatorname{trace}}
\newcommand*{\rank}{\operatorname{rank}}
\newcommand*{\diag}{\operatorname{diag}}
\newcommand*{\vspan}{\operatorname{span}}
\newcommand*{\rowsp}{\operatorname{rowsp}}
\newcommand*{\colsp}{\operatorname{colsp}}
\newcommand*{\svd}{\operatorname{svd}}
\newcommand*{\edm}{\operatorname{edm}}  % euclidean distance matrix (D * D)
\newcommand{\oneblock}[3]{\m{B}_{#1:#2:#3}}
\newcommand{\stripe}[2]{\m{S}_{#1,#2}}

% contingency tables
\newcommand{\abdiff}{\delta_{AB}}

% statistical
\newcommand{\widebar}[1]{\overline{#1}}  
\newcommand{\wig}[1]{\tilde{#1}}  
\newcommand{\bigwig}[1]{\widetilde{#1}}  
\newcommand{\follows}{\sim}  
\newcommand{\leftgiven}{~\left\lvert~}
\newcommand{\given}{~\vert~}
\newcommand{\biggiven}{~\vline~}
\newcommand{\indep}{\bot\hspace{-.6em}\bot}
\newcommand{\notindep}{\bot\hspace{-.6em}\bot\hspace{-0.75em}/\hspace{.4em}}
\newcommand{\depend}{\Join}
\newcommand{\notdepend}{\Join\hspace{-0.9 em}/\hspace{.4em}}
\newcommand{\imply}{\Longrightarrow}
\newcommand{\notimply}{\Longrightarrow \hspace{-1.5em}/ \hspace{0.8em}}
\newcommand{\xyAssociation}{g}
\newcommand{\xDomain}{\mathcal{X}}
\newcommand{\yDomain}{\mathcal{Y}}
\newcommand{\measureRange}{\mathcal{R}}
\newcommand{\bigChi}{\mathcal{D}}
\newcommand{\ind}[2]{I_{#2} \left( #1 \right)}
%\newcommand{\ind}[1]{\mathds{1} \hspace{-0.1cm}\left( #1 \right)}
\newcommand{\mutInf}{\mathcal{I}}
\newcommand{\obscorr}{\widehat{r}^2}
\newcommand{\corr}{r^2}

% operators
\newcommand{\Had}{\circ}
\newcommand{\measureAssociation}{G}
\DeclareMathOperator*{\lmin}{Minimize}
\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\arginf}{arg\,inf}
\DeclareMathOperator*{\argsup}{arg\,sup}

% Sets
\newcommand*{\intersect}{\cap}
\newcommand*{\union}{\cup}
\let\oldemptyset\emptyset
\let\emptyset\varnothing

% Fields, Reals, etc. etc
\newcommand{\field}[1]{\mathbb{#1}}
\newcommand{\Reals}{\field{R}}
\newcommand{\Integers}{\field{Z}}
\newcommand{\Naturals}{\field{N}}
\newcommand{\Complex}{\field{C}}
\newcommand{\Rationals}{\field{Q}}

% Editorial
\newcommand{\needtocite}[1]{{\color{red} [Need to cite {#1} here]}}
\newcommand{\comment}[1]{{\color{steelblue} COMMENT:  {#1}}}
\newcommand{\TODO}[1]{{\color{brickred} TODO:  {#1}}}

\title{On genetic correlation}
\author{Christopher Salahub \\
	\textit{University of Waterloo}}

\begin{document}
	
\maketitle

\section{Introduction} \label{sec:intro}

A structural model of genetics can be constructed which represents the genome of a diplodic individual by a two-column matrix
$$\m{G} = [\ve{g}_1, \ve{g}_2], \text{ } \ve{g}_1, \ve{g}_2 \in \mathcal{B}^{N_P}$$
where $\mathcal{B} = \{\text{adenine, guanine, cytosine, thymine}\}$ is the set of nucleotide bases and $N_P$ is the length of the genome. In humans $N_P \approx 3,234,830,000$. Rather than measuring the whole genome, select $M$ disjoint sequences of interest, called markers, with total length $K$ and record these in
$$\m{S} = [\ve{s}_1 , \ve{s}_2], \text{ } \ve{s}_1, \text{ } \ve{s}_2 \in \mathcal{B}^K.$$
In most cases these disjoint segments are chosen from known single nucleotide polymorphisms, or SNPs, which account for the majority of variation in the coding of the human genome. Typically, SNPs are biallelic, and so take only one of two versions in the population. $\m{S}$ can therefore be summarized into the $M$ SNPs it represents by annotating which allele is present at each location. This can be done using upper- and lowercase letters, for example, to give
\begin{equation} \label{eq:annotMat}
  \m{T} = [\ve{t}_1 , \ve{t}_2], \text{ } \ve{t}_1, \text{ } \ve{t}_2 \in \{A,a\}^M.
\end{equation}
These letters do not represent the same sequence when used at different locations, but rather only indicate which of the two alleles is present at a particular SNP. This annotated matrix serves as the basis of most genetic research, with conventions in notation and modelling going back to \cite{mendel1866} and \cite{fisher1919}.

Genetic research is focused on the heritability of different traits. More directly, this means associating measurable physical traits such as height, eye colour, response to a drug, or the presence of a disease with the entries of $\m{T}$. This must be done in spite of potentially confounding relationships present between different entries in $\m{T}$ due to the process of inheritance itself. To account for inheritance, take the annotated matrices of the parents
\begin{equation} \label{eq:parentAnnot}
  \m{F}_T = [\ve{f}_1 , \ve{f}_2]
, \text{ and }
\m{M}_T = [\ve{m}_1 , \ve{m}_2],
\end{equation}
thereby extending this structural model back a generation. We can now meaningfully talk about inheritance itself. Most crudely, inheritance involves the combination of independently donated variants from each of $\m{F}_T$ and $\m{M}_T$. Two additional processes may perturb the variants: independent assortment and cross overs.

Independent assortment is a well-known phenomenon in genetics, see \cite{siegmundyakir2007}. While Equations \ref{eq:annotMat} and \ref{eq:parentAnnot} present variants as long columns of sequential base pairs, inside of cells these variants are actually organized into separate contiguous sections called chromosomes. Chromosomes within a parent are donated independently of each other. So while offspring may receive the variant from the first column on one chromosome, they can receive the variant from the second column on another. Let $\ve{c}$ be a vector of length $M$ denoting the chromosomal membership of each marker. For marker indices $j$ and $k$, independent assortment means that the variant donated at position $j$ is independent of that at $k$ if $c_j \neq c_k$ within a parent.

Cross overs add additional variation by perturbing sections for which $\ve{c}$ is constant. Within chromosomes, it is possible for the variants to physically cross at a base pair and swap between variants the sections after this cross to the end of the chromosome. This can actually occur several times on the same chromosome. In $\m{T}$ cross overs result in swaps of sections of the columns where $\ve{c}$ is constant. Cross overs recombine the genome to create completely new variants.

\section{Genetic correlation} \label{sec:corr}

To quantify association some measure of association must be applied to $\m{T}$, $\m{F}_T$, and $\m{M}_T$. This quantification is a primary goal of genome-wide association studies, see \cite{uffelmannetal2021gwas, tametal2019benefits, wangetal2005gwas}. While many of the measures in \cite{goodmankruskal1979measures} could be used directly, a more common approach is to encode and summarize these annotated matrices numerically and compute \textbf{observed sample} correlations, given by
\begin{equation} \label{eq:sampleCorr}
  \obscorr(\ve{x}, \ve{y}) = \frac{\sum_{i = 1}^n x_i y_i - n \bar{x} \bar{y}}{\sqrt{\left (\sum_{i = 1}^n x_i^2 - n \bar{x}^2 \right ) \left (\sum_{i = 1}^n y_i^2 - n \bar{y}^2 \right )}}
\end{equation}
for $\ve{x} \in \Reals^n$ and $\ve{y} \in \Reals^n$. If $\ve{x}$ and $\ve{y}$ are treated as realizations of the random variables $X$ and $Y$ respectively, this is the sample estimate of the \textbf{theoretical} correlation
\begin{equation} \label{eq:theorCorr}
 \corr(X, Y) = \frac{Cov(X, Y)}{\sqrt{Var(X) Var(Y)}}
\end{equation}
These can then be used to understand the structure of the genome and its relation to physical traits, as in \cite{poolr, LiJi2005, nyholt2004, cheverudetal2001}.

One possibility is the additive encoding and summary. First, for all of $\m{T}, \m{F}_T,$ and $\m{M}_T$ $A$ is replaced by 1 and $a$ by 0. Row-wise addition of this indicator of $A$ can then be performed on $\m{T}$ to obtain the vector
$$\tr{[z_1, z_2, \dots, z_M]} \in \{0,1,2\}^M.$$
Repeating this for every individual in a population gives $n$ such vectors. Equivalently, we obtain $n$ observations of each of the $M$ markers. Denote the $j^{\text{th}}$ marker measurement on the $i^{\text{th}}$ individual as $z_{ij}$, then measurements over a population can be placed in the $n \times M$ matrix
$$\m{Z} = \begin{bmatrix}
  z_{11} & z_{12} & \dots & z_{1M} \\
  z_{21} & z_{22} & \dots & z_{2M} \\
  \vdots & \vdots & \ddots & \vdots \\
  z_{n1} & z_{n2} & \dots & z_{nM}
\end{bmatrix} := [\ve{z}_1, \ve{z}_2, \dots, \ve{z}_M],$$
where
$$\ve{z}_j = \tr{[z_{1j}, z_{2j}, \dots, z_{Mj}]}.$$
Each $\ve{z}_j$ is a vector of $n$ realizations of the random variable $Z_j$ distributed according to the distribution of the $j^{\text{th}}$ marker in the population. When considering a wild population, this distribution is likely to be unknown. If $\m{M}_T$ and $\m{F}_T$ are known constants for the entire population, however, the distribution of $Z_j$ is dictated by cross overs and independent assortment alone. In either case $\m{Z}$ has an \textbf{observed} pairwise correlation matrix
$$\widehat{\m{R}} = \begin{bmatrix}
  Var(\ve{z}_1) & \obscorr(\ve{z}_1, \ve{z}_2) & \obscorr(\ve{z}_1, \ve{z}_3) & \dots & \obscorr(\ve{z}_1, \ve{z}_{M-1}) & \obscorr(\ve{z}_1, \ve{z}_M) \\
  \obscorr(\ve{z}_2, \ve{z}_1) & Var(\ve{z}_2) & \obscorr(\ve{z}_2, \ve{z}_3) & \dots & \obscorr(\ve{z}_2, \ve{z}_{M-1}) & \obscorr(\ve{z}_2, \ve{z}_M) \\
  \obscorr(\ve{z}_3, \ve{z}_1) & \obscorr(\ve{z}_3, \ve{z}_2) & Var(\ve{z}_3) & \dots & \obscorr(\ve{z}_3, \ve{z}_{M-1}) & \obscorr(\ve{z}_3, \ve{z}_M) \\
  \vdots & \vdots & \vdots & \ddots & \vdots & \vdots \\
  \obscorr(\ve{z}_M, \ve{z}_1) & \obscorr(\ve{z}_M, \ve{z}_2) & \obscorr(\ve{z}_M, \ve{z}_3) & \dots & \obscorr(\ve{z}_M, \ve{z}_{M-1}) & Var(\ve{z}_M) \\
\end{bmatrix}.$$
For an arbitrary entry in this matrix, say $\obscorr(\ve{z}_j, \ve{z}_k)$, let $c_j$ and $c_k$ indicate the respective chromosomes of markers $j$ and $k$. and suppose that these markers have a probability of recombination of $p_r$. If we assume that
\begin{itemize}
\item the population are all offspring of the known matrices $\m{M}_T$ and $\m{F}_T$;
\item cross overs and independent assortment are the only sources of recombination;
\item cross overs occur with perfect alignment across variants; and
\item cross overs occur independently within chromosomes
\end{itemize}
then it can be shown that the \textbf{expected} theoretical correlation is given by
\begin{equation} \label{eq:prGenCorr}
  \corr(Z_j, Z_k) = \ind{c_k}{c_j} \gamma ( 1 - 2p_r )
\end{equation}
where
\begin{equation*}\ind{x}{y} = \begin{cases}
  1, & \text{ if } x = y \\
  0, & \text{ otherwise}
\end{cases}
\end{equation*}
is the indicator function and $\gamma \in \{-1, -1/\sqrt{2}, 0, 1/\sqrt{2}, 1\}$ is a constant depending on the entries of $\m{M}_T$ and $\m{F}_T$. Ignoring the other markers which do not contribute to this pairwise correlation, the encodings of the annotated matrices $\m{M}_T$ and $\m{F}_T$ can be written
\begin{equation} \label{eq:encodedParents}
  \m{F}_X = \begin{bmatrix}
  f_{j1} & f_{j2} \\
  f_{k1} & f_{k2} \\
\end{bmatrix} \text{ and }
\m{M}_X = \begin{bmatrix}
  m_{j1} & m_{j2} \\
  m_{k1} & m_{k2} \\
\end{bmatrix},
\end{equation}
where all entries are 0 or 1. $\gamma$ is most succinctly defined using the difference matrix
\begin{equation} \label{eq:diffMatrix}
  \sm{\Delta} = \begin{bmatrix}
    f_{j1} - f_{j2} & m_{j1} - m_{j2} \\
    f_{k1} - f_{k2} & m_{k1} - m_{k2} \\
  \end{bmatrix} := \begin{bmatrix}
    \delta_{jF} & \delta_{jM} \\
    \delta_{kF} & \delta_{kM}
  \end{bmatrix}
\end{equation}
recording the difference between the two variants in each parent at each marker position. Using these differences
$$\gamma = \frac{ \Big [ \delta_{jF} \delta_{kF} + \delta_{jM} \delta_{kM} \Big ] }{  \sqrt{ \big ( \delta_{jF}^2 + \delta_{jM}^2 \big ) \big ( \delta_{kF}^2 + \delta_{kM}^2 \big )}}.$$

If we additionally assume that cross overs occur uniformly over the interval $j$ to $k$ and that this interval is sufficiently large, the map distance of \cite{haldane1919} arises automatically from this model. Supposing the interval $j$ to $k$ has an arbitrary length $d(j,k)$ measured in reference to a uniform recombination rate $\beta \in \Reals$, the probability of recombination is given by
$$p_r(d(j,k)) = \frac{1}{2} \left ( 1 - e^{-2\beta d(j,k)} \right )$$
and so
\begin{equation} \label{eq:GenCorr}
  \corr(Z_j, Z_k) = \ind{c_k}{c_j} \gamma e^{-2 \beta d(j,k)}.
\end{equation}

\section{The distribution of correlation} \label{sec:distribution}

While the point estimates of Equations \ref{eq:GenCorr} and \ref{eq:prGenCorr} are useful to understand the mean behaviour of the correlations for large populations, they do not communicate the theoretical distributions. This makes an assesment of fit for any particular $\widehat{\m{R}}$ difficult, requiring repeated simulations to obtain an empirical null distribution. As this results from treating $\m{M}_X$ and $\m{F}_X$ as constant in Equation \ref{eq:encodedParents}, the random matrices
\begin{equation} \label{eq:encodedParents}
  \mathcal{F}_X = \begin{bmatrix}
  F_{j1} & F_{j2} \\
  F_{k1} & F_{k2} \\
\end{bmatrix} \text{ and }
\mathcal{M}_X = \begin{bmatrix}
  M_{j1} & M_{j2} \\
  M_{k1} & M_{k2} \\
\end{bmatrix},
\end{equation}
resolve this problem. Each of the entries in these matrices can be treated as a Bernoulli random variable with some probability of being 1. The matrices $\m{M}_X$ and $\m{F}_X$ can be repurposed to parametrize these variables by giving the expectations of each, thereby generalizing the constant case.

Additional random variables can be introduced to account for cross overs and the equal probability of either variant being donated. Let $L_F$ and $L_M$ be Bernoulli random variables which are 1 if the left column is donated by the corresponding random matrix. Similarly take $C_F$ and $C_M$ as Bernoulli random variables which are 1 if recombination due to cross overs occurs in the corresponding random matrix. Therefore
\begin{equation} \label{eq:LFLM}
  L_F, L_M \sim Bern \left ( \frac{1}{2} \right )
\end{equation}
with $L_F \indep L_M$, and
\begin{equation} \label{eq:CFCM}
  C_F, C_M \sim Bern \left ( p_r(d(j,k)) \right )
\end{equation}
with $C_F \indep C_M$.

Denoting the relevant marker encodings of the offspring of $\mathcal{F}_X$ and $\mathcal{M}_X$ by
\begin{equation} \label{eq:randomOff}
  \mathcal{X} = \begin{bmatrix}
    X_{j1} & X_{j2} \\
    X_{k1} & X_{k2}
  \end{bmatrix},
\end{equation}
and letting the second row swap columns in the event of a cross over, we get the stochastic representations
\begin{align*}
  X_{j1} & =  (1 - C_F)\Big [L_F F_{j1} + (1 - L_F) F_{j2} \Big ] + C_F \Big [ L_F F_{j1} + (1 - L_F) F_{j2} \Big ] \\
  X_{j2} & =  (1 - C_M)\Big [L_M M_{j1} + (1 - L_M) M_{j2} \Big ] + C_M \Big [ L_M M_{j1} + (1 - L_M) M_{j2} \Big ] \\
  X_{k1} & =  (1 - C_F)\Big [L_F F_{k1} + (1 - L_F) F_{k2} \Big ] + C_F \Big [ (1 - L_F) F_{k1} + L_F F_{k2} \Big ] \\
  X_{k2} & =  (1 - C_M)\Big [L_M M_{k1} + (1 - L_M) M_{k2} \Big ] + C_M \Big [ (1 - L_M) M_{k1} + L_M M_{k2} \Big ] 
\end{align*}
which correspond to the values in the cases of left and right donation both with and without recombination. These terms can be simplified by collecting terms based on the entries of $\mathcal{F}_X$ and $\mathcal{M}_X$. Using $X_{j1}$ and $X_{j2}$, for example:
\begin{align*}
  X_{j1} & = L_F  F_{j1} + (1 - L_F) F_{j2}
\end{align*}
and
\begin{align*}
  X_{j2} & = \Big ( L_F + C_F - 2 C_F L_F \Big ) F_{k1} + \Big ( 1 - L_F - C_F + 2 C_F L_F \Big ) F_{k2} \\
         & = \Big ( L_F^2 + C_F^2 - 2 C_F L_F \Big ) F_{k1} + \Big ( 1 - L_F^2 - C_F^2 + 2 C_F L_F \Big ) F_{k2} \\
  & = ( L_F - C_F )^2 F_{j1} + \Big [ 1 - ( L_F - C_F )^2 \Big ] F_{j2} 
\end{align*}
as $L_F, C_F \in \{0,1\}$. This can be simplified further by considering $(L_F - C_F)^2 := D_F$. As $L_F - C_F \in \{-1,0,1\}$ is a random difference, $D_F \in \{0, 1\}$ is a Bernoulli random variable. More specifically, considering the probability of $L_F - C_F \in \{-1, 1\}$ gives
$$D_F \sim Bern \left( \frac{p_r(d(j,k))}{2} + \frac{1}{2}( 1 - p_r(d(j,k)) ) \right ) = Bern \left ( \frac{1}{2} \right ).$$
This is unsurprising. Cross overs affect both variants symmetrically, and so there is no reason to expect one to be favoured over the other in inheritance. Extending this logic to the other entries of $\mathcal{X}$ and defining $D_M$ similarly to $D_F$ gives
\begin{align*}
  X_{j1} & =  L_F F_{j1} + (1 - L_F) F_{j2} \\ %\label{eq:xj1}\\
  X_{j2} & =  L_M M_{j1} + (1 - L_M) M_{j2} \\ %\label{eq:xj2}\\
  X_{k1} & =  D_F F_{k1} + (1 - D_F) F_{k2} \\ %\label{eq:xk1}\\
  X_{k2} & =  D_M M_{k1} + (1 - D_M) M_{k2} %\label{eq:xk2}.
\end{align*}
Furthermore, $L_F \indep D_F$ and $L_M \indep D_M$ as the conditional distributions are the same as the unconditional ones. Noting that the random variables in $\mathcal{F}$ and $\mathcal{M}$ are all within $\{0,1\}$, each of these is a mixture of two Bernoulli random variables.

The summary vector has entries
\begin{align*}
  Z_1 & = X_{j1} + X_{j2} \\
  Z_2 & = X_{k1} + X_{k2}
\end{align*}
and so understanding the distribution of the entries of $\mathcal{X}$ is key to characterizing the distribution of this summary. First note that $L_F, L_M, D_F,$ and $D_M$ are all mutually independent and are assumed to be independent of $\mathcal{F}$ and $\mathcal{M}$. While it is possible the variant present affects the probability of recombination, such complexity would have a minor effect at most. Therefore
\begin{align}
  E[X_{j1}] & =  \frac{1}{2} \Big ( E[F_{j1}] + E[F_{j2}] \Big ) \label{eq:Exj1}\\ 
  E[X_{j2}] & =  \frac{1}{2} \Big ( E[M_{j1}] + E[M_{j2}] \Big ) \label{eq:Exj2} \\ 
  E[X_{k1}] & =  \frac{1}{2} \Big ( E[F_{k1}] + E[F_{k2}] \Big ) \label{eq:Exk1} \\ 
  E[X_{k2}] & =  \frac{1}{2} \Big ( E[M_{k1}] + E[M_{k2}] \Big ) \label{eq:Exk2} 
\end{align}
where the expectations of each entry of $\mathcal{M}$ and $\mathcal{F}$ are population parameters. Using the conditional variance decomposition
\begin{align*}
  Var(X_{j1}) & = E \Big [ Var(X_{j1} | L_F) \Big ] + Var \Big ( E[X_{j1} | L_F] \Big ) \\
              & = \frac{1}{2} \Big [ Var(F_{j1}) + Var(F_{j2}) \Big ] + \frac{1}{2} \Big ( E[F_{j1}]^2 + E[F_{j2}]^2 \Big ) - \frac{1}{4} \Big ( E[F_{j1}] + E[F_{j2}] \Big )^2 \\
              & = Var(F_{j1}) + Var(F_{j2}) + \frac{1}{4} E[F_{j1}]^2 + \frac{1}{4} E[F_{j2}]^2 - \frac{1}{2} E[F_{j1}] E[F_{j2}], \\
              & = Var(F_{j1}) + Var(F_{j2}) + \frac{1}{4} \Big ( E[F_{j1}] - E[F_{j2}] \Big )^2
\end{align*}
and so
\begin{align}
  Var(X_{j1}) & = Var(F_{j1}) + Var(F_{j2}) - \frac{1}{4} \Big ( E[F_{j1}] - E[F_{j2}] \Big )^2 \label{eq:varxj1} \\
  Var(X_{j2}) & = Var(M_{j1}) + Var(M_{j2}) - \frac{1}{4} \Big ( E[M_{j1}] - E[M_{j2}] \Big )^2 \label{eq:varxj2} \\
  Var(X_{k1}) & = Var(F_{k1}) + Var(F_{k2}) - \frac{1}{4} \Big ( E[F_{k1}] - E[F_{k2}] \Big )^2 \label{eq:varxk1} \\
  Var(X_{k2}) & = Var(M_{k1}) + Var(M_{k2}) - \frac{1}{4} \Big ( E[M_{k1}] - E[M_{k2}] \Big )^2 \label{eq:varxk2}.
\end{align}
Finally, we can consider the covariances. Start with
\begin{align*}
  E[X_{j1} X_{j2}] & = E \Big [ \big ( L_F F_{j1} + [1 - L_F] F_{j2} \big ) \big ( L_M M_{j1} + [1 - L_M] M_{j2} \big ) \Big ] \\
                    & = E \Bigg ( E \Big [ \big ( L_F F_{j1} + [1 - L_F] F_{j2} \big ) \big ( L_M M_{j1} + [1 - L_M] M_{j2} \big ) \Big | L_F, L_M \Big ] \Bigg )\\
  & = \frac{1}{4} \Big ( E[F_{j1} M_{j1}] + E[F_{j1} M_{j2}] + E[F_{j2} M_{j1}] + E[F_{j2} M_{j2}] \Big ),
\end{align*}
and from Equations \ref{eq:Exj1} and \ref{eq:Exj2}
\begin{align*}
  E[X_{j1}] E[X_{j2}] & = \frac{1}{4} \Big ( E[F_{j1}] + E[F_{j2}] \Big ) \Big ( E[M_{j1}] + E[M_{j2}] \Big ) \\
  & = \frac{1}{4} \Big ( E[F_{j1}] E[M_{j1}] + E[F_{j1}] E[M_{j2}] + E[F_{j2}] E[M_{j1}] + E[F_{j2}] E[M_{j2}] \Big ).
\end{align*}
Combining these results gives
\begin{align*}
  Cov(X_{j1}, X_{j2}) & = E[X_{j1} X_{j2}] - E[X_{j1}] E[X_{j2}] \\
                      & = \frac{1}{4} \Big [ Cov(F_{j1}, M_{j1}) + Cov(F_{j1}, M_{j2}) + Cov(F_{j2}, M_{j1}) + Cov(F_{j2}, M_{j2}) \Big ]
\end{align*}
Noting that these two values are from different parents, and so are on different variants, we next consider two on the same variant:
\begin{align*}
  E[X_{j1} X_{k1}] & = E \Big [ \big ( L_F F_{j1} + [1 - L_F] F_{j2} \big ) \big ( D_F F_{k1} + [1 - D_F] F_{k2} \big ) \Big ] \\
                    & = E \Bigg ( E \Big [ \big ( L_F F_{j1} + [1 - L_F] F_{j2} \big ) \big ( D_F F_{k1} + [1 - D_F] F_{k2} \big ) \Big | L_F, D_F \Big ] \Bigg )\\
  & = \frac{1}{4} \Big ( E[F_{j1} F_{k1}] + E[F_{j1} F_{k2}] + E[F_{j2} F_{k1}] + E[F_{j2} F_{k2}] \Big ),
\end{align*}
and
\begin{align*}
  E[X_{j1}] E[X_{k1}] & = \frac{1}{4} \Big ( E[F_{j1}] + E[F_{j2}] \Big ) \Big ( E[F_{k1}] + E[F_{k2}] \Big ) \\
  & = \frac{1}{4} \Big ( E[F_{j1}] E[F_{k1}] + E[F_{j1}] E[F_{k2}] + E[F_{j2}] E[F_{k1}] + E[F_{j2}] E[F_{k2}] \Big ).
\end{align*}
And so we obtain the set of equations
\begin{align}
  Cov(X_{j1}, X_{j2}) & = \frac{1}{4} \Big [ Cov(F_{j1}, M_{j1}) + Cov(F_{j1}, M_{j2}) + Cov(F_{j2}, M_{j1}) + Cov(F_{j2}, M_{j2}) \Big ] \label{eq:covj1j2} \\
  Cov(X_{j1}, X_{k1}) & = \frac{1}{4} \Big [ Cov(F_{j1}, F_{k1}) + Cov(F_{j1}, F_{k2}) + Cov(F_{j2}, F_{k1}) + Cov(F_{j2}, F_{k2}) \Big ] \label{eq:covj1j2} \\
  Cov(X_{j1}, X_{k2}) & = \frac{1}{4} \Big [ Cov(F_{j1}, M_{k1}) + Cov(F_{j1}, M_{k2}) + Cov(F_{j2}, M_{k1}) + Cov(F_{j2}, M_{k2}) \Big ] \label{eq:covj1j2} \\
\end{align}

From Equations \ref{eq:z1} and \ref{eq:z2}, we get
\begin{align}
  E[Z_1] & =  \frac{1}{2} \Big ( E[F_{j1}] + E[F_{j2}] + E[M_{j1}] + E[M_{j2}] \Big ) \label{eq:Expz1} \\
  E[Z_2] & =  \frac{1}{2} \Big ( E[F_{k1}] + E[F_{k2}] + E[M_{k1}] + E[M_{k2}] \Big ). \label{eq:Expz2}
\end{align}
While we can decompose the variance conditionally as
\begin{align*}
  Var(Z_1) & = E \Big [ Var(Z_1 | D_F, D_M) \Big ] + Var \Big ( E [ Z_1 | D_F, D_M ] \Big ).
\end{align*}
\begin{align*}
  E \Big [ Var(Z_1 | D_F, D_M) \Big ] & = \frac{1}{4} Var(F_{j1} + M_{j1}) + \frac{1}{4} Var(F_{j1} + M_{j2}) + \frac{1}{4} Var(F_{j2} + M_{j1}) + \frac{1}{4} Var(F_{j2} + M_{j2})
\end{align*}

\bibliographystyle{plainnat}
\renewcommand*{\bibname}{References} % use title "References" for bibliography
\bibliography{../Bibliography/fullbib}

\end{document}
