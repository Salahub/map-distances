\documentclass{article}

% For PDF, suitable for double-sided printing, change the PrintVersion variable below to "true" and use this \documentclass line instead of the one above:
%\documentclass[letterpaper,12pt,titlepage,openright,twoside,final]{book}
\newcommand{\package}[1]{\textbf{#1}} % package names in bold text
\newcommand{\cmmd}[1]{\textbackslash\texttt{#1}} % command name in tt font 
\newcommand{\href}[1]{#1} % does nothing, but defines the command so the print-optimized version will ignore \href tags (redefined by hyperref pkg).
%\newcommand{\texorpdfstring}[2]{#1} % does nothing, but defines the command
% Anything defined here may be redefined by packages added below...

% This package allows if-then-else control structures.
\usepackage{ifthen}
\newboolean{PrintVersion}
\setboolean{PrintVersion}{false}
% CHANGE THIS VALUE TO "true" as necessary, to improve printed results for hard copies by overriding some options of the hyperref package, called below.

%\usepackage{nomencl} % For a nomenclature (optional; available from ctan.org)
\usepackage{amsmath,amssymb,amstext} % Lots of math symbols and environments
\usepackage[pdftex]{graphicx} % For including graphics N.B. pdftex graphics driver
\usepackage{amsmath,amssymb,amstext,amsthm,amsfonts}
\usepackage{dsfont}
\usepackage[pdftex]{graphicx}
\usepackage{caption}
\usepackage{color}% Include colors for document elements
\usepackage{dcolumn}% Align table columns on decimal point
\usepackage{bm}% bold math
\usepackage{float}
\usepackage{multirow}
\usepackage[round]{natbib}   % omit 'round' option for square brackets

\usepackage{algorithm} % For counting chapters
\usepackage{algorithmicx, algpseudocode}
%\renewcommand{\algorithmiccomment}[1]{// #1} % Brackets are confused with the sets
%\algsetup{linenosize=\scriptsize}

% N.B. HYPERREF MUST BE THE LAST PACKAGE LOADED; ADD ADDITIONAL PKGS ABOVE
\usepackage[pdftex,pagebackref=false]{hyperref} % with basic options
%\usepackage[pdftex,pagebackref=true]{hyperref}
% N.B. pagebackref=true provides links back from the References to the body text. This can cause trouble for printing.
% define colours
\definecolor{background-color}{gray}{0.98}
\definecolor{steelblue}{rgb}{0.27, 0.51, 0.71}
\definecolor{brickred}{rgb}{0.8, 0.25, 0.33}
\definecolor{bluegray}{rgb}{0.4, 0.6, 0.8}
\definecolor{amethyst}{rgb}{0.6, 0.4, 0.8}

\hypersetup{
	plainpages=false,       % needed if Roman numbers in frontpages
	unicode=false,          % non-Latin characters in Acrobat's bookmarks
	pdftoolbar=true,        % show Acrobats toolbar?
	pdfmenubar=true,        % show Acrobat's menu?
	pdffitwindow=false,     % window fit to page when opened
	pdfstartview={FitH},    % fits the width of the page to the window
	pdftitle={Genetic Model},    % title: CHANGE THIS TEXT!
	pdfauthor={Chris Salahub},    % author: CHANGE THIS TEXT! and uncomment this line
	%pdfsubject={Statistics},  % subject: CHANGE THIS TEXT! and uncomment this line
	%    pdfkeywords={keyword1} {key2} {key3}, % list of keywords, and uncomment this line if desired
	pdfnewwindow=true,      % links in new window
	colorlinks=true,        % false: boxed links; true: colored links
	linkcolor=steelblue,         % color of internal links
	citecolor=brickred,        % color of links to bibliography
	filecolor=magenta,      % color of file links
	urlcolor=cyan           % color of external links
}
\ifthenelse{\boolean{PrintVersion}}{   % for improved print quality, change some hyperref options
	\hypersetup{	% override some previously defined hyperref options
		%    colorlinks,%
		citecolor=black,%
		filecolor=black,%
		linkcolor=black,%
		urlcolor=black}
}{} % end of ifthenelse (no else)

%\usepackage[automake,toc,abbreviations]{glossaries-extra} % Exception to the rule of hyperref being the last add-on package

% Page margins
% uWaterloo thesis requirements specify a minimum of 1 inch (72pt) margin at the
% top, bottom, and outside page edges and a 1.125 in. (81pt) gutter margin (on binding side). 
\setlength{\marginparwidth}{0pt} % width of margin notes
% N.B. If margin notes are used, you must adjust \textwidth, \marginparwidth
% and \marginparsep so that the space left between the margin notes and page
% edge is less than 15 mm (0.6 in.)
\setlength{\marginparsep}{0pt} % width of space between body text and margin notes
\setlength{\evensidemargin}{0.125in} % Adds 1/8 in. to binding side of all even pages when "twoside" is selected
\setlength{\oddsidemargin}{0.125in} % Adds 1/8 in. to the left of all pages when "oneside" is selected,
% and to the left of all odd pages when "twoside" is selected
\setlength{\textwidth}{6.375in} % assuming US letter paper (8.5 in. x 11 in.) and margins as above
\raggedbottom

\setlength{\parskip}{\medskipamount} % space between paragraphs
\renewcommand{\baselinestretch}{1} % line space setting

% Commands
% Code
\newcommand{\code}[1]{\texttt{#1}}
\newcommand*{\Rnsp}{\textsf{R}}
\newcommand*{\R}{\textsf{R}$~$}
\newcommand*{\Pythonnsp}{\textsf{Python}}
\newcommand*{\Python}{\textsf{Python}$~$}
\newcommand{\pkg}[1]{\textsf{#1}}
\newcommand{\pkgsp}[1]{\textsf{#1}$~$}
\algblock{Input}{EndInput}
\algnotext{EndInput}
\newcommand{\Desc}[2]{\State \makebox[2em][l]{#1}#2}

% Theorem styles
\newtheorem{definition}{Definition}
\newtheorem{theorem}{Theorem}

% vectors
\newcommand{\ve}[1]{\mathbf{#1}}           % for vectors
\newcommand{\sv}[1]{\boldsymbol{#1}}   % for greek letters
\newcommand{\m}[1]{\mathbf{#1}}               % for matrices
\newcommand{\sm}[1]{\boldsymbol{#1}}   % for greek letters
\newcommand{\tr}[1]{{#1}^{\mkern-1.5mu\mathsf{T}}}              % for transpose
\newcommand{\conj}[1]{{#1}^{\ast}}
\newcommand{\norm}[1]{||{#1}||}              % norm
\newcommand{\frob}[1]{\norm{#1}_F}
\newcommand{\abs}[1]{\lvert{#1}\rvert}              % norm
\newcommand*{\mvec}{\operatorname{vec}}
\newcommand*{\trace}{\operatorname{trace}}
\newcommand*{\rank}{\operatorname{rank}}
\newcommand*{\diag}{\operatorname{diag}}
\newcommand*{\vspan}{\operatorname{span}}
\newcommand*{\rowsp}{\operatorname{rowsp}}
\newcommand*{\colsp}{\operatorname{colsp}}
\newcommand*{\svd}{\operatorname{svd}}
\newcommand*{\edm}{\operatorname{edm}}  % euclidean distance matrix (D * D)
\newcommand{\oneblock}[3]{\m{B}_{#1:#2:#3}}
\newcommand{\stripe}[2]{\m{S}_{#1,#2}}

% contingency tables
\newcommand{\abdiff}{\delta_{AB}}

% statistical
\newcommand{\widebar}[1]{\overline{#1}}  
\newcommand{\wig}[1]{\tilde{#1}}  
\newcommand{\bigwig}[1]{\widetilde{#1}}  
\newcommand{\follows}{\sim}  
\newcommand{\leftgiven}{~\left\lvert~}
\newcommand{\given}{~\vert~}
\newcommand{\biggiven}{~\vline~}
\newcommand{\indep}{\bot\hspace{-.6em}\bot}
\newcommand{\notindep}{\bot\hspace{-.6em}\bot\hspace{-0.75em}/\hspace{.4em}}
\newcommand{\depend}{\Join}
\newcommand{\notdepend}{\Join\hspace{-0.9 em}/\hspace{.4em}}
\newcommand{\imply}{\Longrightarrow}
\newcommand{\notimply}{\Longrightarrow \hspace{-1.5em}/ \hspace{0.8em}}
\newcommand{\xyAssociation}{g}
\newcommand{\xDomain}{\mathcal{X}}
\newcommand{\yDomain}{\mathcal{Y}}
\newcommand{\measureRange}{\mathcal{R}}
\newcommand{\bigChi}{\mathcal{D}}
\newcommand{\ind}[2]{I_{#2} \left( #1 \right)}
%\newcommand{\ind}[1]{\mathds{1} \hspace{-0.1cm}\left( #1 \right)}
\newcommand{\mutInf}{\mathcal{I}}

% operators
\newcommand{\Had}{\circ}
\newcommand{\measureAssociation}{G}
\DeclareMathOperator*{\lmin}{Minimize}
\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\arginf}{arg\,inf}
\DeclareMathOperator*{\argsup}{arg\,sup}

% Sets
\newcommand*{\intersect}{\cap}
\newcommand*{\union}{\cup}
\let\oldemptyset\emptyset
\let\emptyset\varnothing

% Fields, Reals, etc. etc
\newcommand{\field}[1]{\mathbb{#1}}
\newcommand{\Reals}{\field{R}}
\newcommand{\Integers}{\field{Z}}
\newcommand{\Naturals}{\field{N}}
\newcommand{\Complex}{\field{C}}
\newcommand{\Rationals}{\field{Q}}

% Editorial
\newcommand{\needtocite}[1]{{\color{red} [Need to cite {#1} here]}}
\newcommand{\comment}[1]{{\color{steelblue} COMMENT:  {#1}}}
\newcommand{\TODO}[1]{{\color{brickred} TODO:  {#1}}}

\title{Using a structural genetic model to derive map distances and correlation}
\author{Chris Salahub \\
	\textit{University of Waterloo}}

\begin{document}
	
\maketitle

\section{Introduction} \label{sec:intro}

Genetic research today routinely considers the entirety of a genome to identify the regions most strongly related to measured physical traits. The goal is to associate measured genome sequences, the \emph{genotype}, with physical characteristics, the \emph{phenotype}. Computational and methodological advances in the pursuit of these \emph{quantitative trait loci} (QTLs) have distinguished \emph{genomics} as its own field. Central to genomics is the \emph{genome-wide association study} (GWAS), where many \emph{markers}, sequences of nucleotides at known positions on the genome, are measured. Extracting useful results from markers and measured traits is a complicated task which has motivated decades of statistical and biological research. Interested individuals are left to sort through this research in order to understand genomics.

\begin{figure}[!ht]
  \begin{center}
  \includegraphics[scale = 1]{./img/modelDiagram.pdf}
  \caption{A structural model of a GWAS.}
  \label{fig:modelDiagram}
\end{center}
\end{figure}

To aid in this, Figure \ref{fig:modelDiagram} draws on the literature to present a simple structural model of the process taken in GWAS to convert raw marker measurements into a form which can be used to identify QTLs. It identifies four key steps (\emph{selection}, \emph{annotation}, \emph{encoding}, and \emph{summarization}) between five increasingly abstract representations of the genome, where the genome is defined by \cite{doergeetal1997search} as all heritable material potentially passed from parent to offspring. By highlighting the abstractions and steps in plain language, this simplified model provides a convenient map to guide the understanding of GWAS. This is not a replacement for surveys such as \cite{uffelmannetal2021gwas, tametal2019benefits}, instead it provides a guiding structural framework with exceptional explanatory power to facilitate understanding of other papers in the field.

The model starts with $\m{G}$, the whole genome of an individual organism. Genetic information is stored in DNA, a long molecule consisting of a sequence of four \emph{nucleotide bases}: guanine, cytosine, adenine, and thymine. A \emph{diplodic} individual inherits one version or \emph{variant} of a complete DNA sequence from each parent, and so has two copies in all \emph{somatic} (i.e. non-reproductive) cells. Though it can be represented as one long sequence, DNA is actually structured into \emph{chromosomes}, separate strands of DNA which contain only a part of the sequence. As most genetic research concerns diplodic species, this will be implicitly assumed.

It is usually not feasible or desirable to design a study around the measurement of all of $\m{G}$, and so the \emph{select} step chooses regions to measure. These regions are represented by $\m{S}$. Often $\m{S}$ consists of a series of \emph{single nucleotide polymorphisms} (SNPs), single nucleotide substitutions in a known sequence at a known position. In human studies this is supported by SNP databases such as \cite{NCBIdbSNP} which document hundreds of millions of common SNPs in the human genome. Only a small proportion of these are estimated to occur frequently enough in the population to be useful in a GWAS, perhaps 15 million according to \cite{koboldtetal2013next}. Modern SNP arrays can simultaneously identify roughly one million of these per array, see \cite{laframboise2009, tametal2019benefits}, and most GWAS will measure an array's worth of SNPs. \emph{Linkage disequilibrium}, effectively the correlation between regions of the genome, facilitates inference to regions outside of those selected in $\m{S}$. While third generation genome sequencing technologies allow for entire genomes to be sequenced, as noted in \cite{heatherchain2016sequencers, hasinetal2017multi, uffelmannetal2021gwas}, persistent high costs of next generation technologies and more than a decade of SNP microarray development leave microarrays as the dominant measurement method.

After selecting SNPs to obtain $\m{S}$, researchers must \emph{annotate} the raw data. The raw signal produced by an SNP microarray is fluorescence, with different degrees of fluorescence corresponding to a different genotypes. Converting the fluorescent areas of an array to a genotype is a challenging problem and has developed in tandem with the arrays themselves. Early models used non-parametric clustering techniques on the signal from several microarray pores, but more complex hidden Markov and Bayesian models have also been developed. \cite{laframboise2009} details some of these. Whatever method is used, the selected regions are assigned genotypes in $\m{T}$ denoted with capital or lowercase letters at each SNP, as in \cite{siegmundyakir2007, visschergoddard2019}.

Finally, relationships between $\m{T}$ and an observed trait or within $\m{T}$ itself are quantified by converting each annotated SNP to a number. To do this, GWAS first \emph{encode} each SNP variant with a numeric value and then \emph{summarize} the pairs at each location into a number. Typically no distinction is made between these steps: \cite{LanderBotstein1989, cheverud2001, siegmundyakir2007} detail the \emph{dominance} and \emph{additive} summaries by moving directly from a genotype to a numeric value. It is useful for clarity and full generality to separate the two distinct steps involved in this process, however.

This paper presents the details of this structural model. By using mathematical notation for each of the abstractions, a framework with extraordinary explanatory power is devised. Section \ref{sec:theModel} provides a detailed explanation of the model with all the necessary notation. The model is then used in a novel derivation of the Haldane \emph{map distance}, a common measure used to locate SNPs, in Section \ref{sec:derivingDists}. The utility of the model if further demonstrated in Section \ref{sec:correlation}, where the model is used to derive the correlation between markers under classic breeding population settings. This results in a convenient expression of the correlation between markers in any genetic study. Finally, Section \ref{sec:model2real} compares the results of this derivation directly to panel data in mice.

\section{A structural genetic model} \label{sec:theModel}

The structural model starts with
$$\m{G} = [\ve{g}_1| \ve{g}_2], \text{ } \ve{g}_1, \ve{g}_2 \in \mathcal{B}^{N_P}$$
where $\mathcal{B} = \{\text{adenine, guanine, cytosine, thymine}\}$ is the set of nucleotide bases and $N_P$ is the length of the genome, in humans $N_P = 3,234,830,000$. $\m{G}$ represents the whole genome of an individual, with both the maternal and paternal variants of all chromosomes placed sequentially in adjacent columns. Both of these variants are complete, double-stranded sequences of DNA, but nucleotides pair uniquely. Adenine binds exclusively with thymine and guanine exclusively binds with cytosine. Therefore $\ve{g}_1$ and $\ve{g}_2$ record the pattern only for one of the two DNA strands for each column, the complementary strand being implied by this sequence.

Rather than address the whole genome, GWAS typically deal with a selected subset of segments on chromosomes of interest. This is represented by
$$\m{S} = [\ve{s}_1 | \ve{s}_2], \text{ } \ve{s}_1, \text{ } \ve{s}_2 \in \mathcal{B}^K$$
with $K \ll N_P$. This corresponds to a mapping from $\m{G} \rightarrow \m{S}$ where $K$ rows of $\m{G}$ are chosen or sampled to create $\m{S}$. These rows are not typically selected at random, but are motivated by previous work and databases of SNPs and other known markers. Most commonly, then, the mapping $\m{G} \rightarrow \m{S}$ is a selection of $M < K$ disjoint sequences from $\m{G}$. 

In the case of SNPs, the markers are most often \textit{biallelic}, i.e. the population is dominated by two different sequences or \textit{alleles} at the marker. These are often denoted using two different letters, such as $A$ and $B$, or analogously the uppercase and lowercase version of the same letter, such as $A$ and $a$. Converting the measured markers to letters is called annotation, which maps $\m{S} \rightarrow \m{T}$ with
$$\m{T} = [\ve{t}_1 | \ve{t}_2], \text{ } \ve{t}_1, \text{ } \ve{t}_2 \in \{A,a\}^M.$$
Denoting the $i^{\text{th}}$ position of $\ve{t}_j$ as $t_{ij}$, $t_{lj} = A$ and $t_{mj} = A$ do not represent identical sequences at positions $l$ and $m$. Instead this indicates that the sequences annotated by the capital at each position are present at their respective positions.

These annotated variants in $\m{T}$ might next be converted to a numeric form. This is a mapping $\m{T} \rightarrow \m{X}$ such that
$$\m{X} := [\ve{x}_1 | \ve{x}_2], \text{ } \ve{x}_1, \text{ } \ve{x}_2 \in \Reals^M.$$
Commonly this is even more restricted with $\ve{x}_j \in \{0,1\}^M$ where
\begin{equation} \label{eq:indicator}
x_{ij} = \begin{cases}
  1, & \text{ if } t_{ij} = A \\
  0, & \text{ if } t_{ij} = a
\end{cases},
\end{equation}
is an indicator of the presence of the allele denoted with a capital.

Finally, $\m{X}$ may be converted into a vector
$$\ve{z} \in \Reals^M$$
summarizing the individual's inherited variants. There are many common mappings $\m{X} \rightarrow \ve{z}$. The \textit{dominance mapping} takes $z_i = \max\{x_{i1}, x_{i2}\}$, the \textit{homozygous mapping} uses $z_i = I_{x_{i1} = x_{i2}}$, and the \textit{additive map} is $\ve{z} = \ve{x}_1 + \ve{x}_2$, where $\ve{x}_1$ and $\ve{x}_2$ are given according to Equation \ref{eq:indicator}. The additive map gives $\ve{z} \in \{0,1,2\}^M$, and $z_i$ is equal to the count of copies of $A$ at the $i^{\text{th}}$ marker across both of an individual's variants.

This entire toy model is displayed in Figure \ref{fig:modelDiagram}, with descriptive names added to each mapping. In the first step, $\m{G} \rightarrow \m{S}$, we \textit{select} segments of the entire genome to obtain the marker sequences or interest. The next step, $\m{S} \rightarrow \m{T}$, \textit{annotates} the chosen markers by indicating which of the common alleles is present for that marker. These annotations are then converted to numeric values, or \textit{encoded}, in the step $\m{T} \rightarrow \m{X}$. Finally, we \textit{summarize} the matrix $\m{X}$ into a vector $\ve{z}$ with some row-wise operation. Each of these italicized steps could be performed in a number of ways, with consequences on the final quantification of the genome's relevant features.

Recall the motivation of the exercise, that of identifying the number of effective tests based on the relationships between them. It should be noted, then, that the numerization and summarization steps are not strictly necessary. Categorical measures of association, such as the $\chi^2$ test, could readily be applied to $\m{T}$, where each possible row combination is treated as a different category. Such measurement would likely be more computationally inefficient, but would entirely circumvent the last two steps of Figure \ref{fig:modelDiagram}. 

\section{Deriving map distance} \label{sec:derivingDists}

The typical derivation of map distance is based on Haldane's original differential equation, as in \cite{kosambi1943estimation} and \cite{xu2013principles}. A novel derivation based on the model from Figure \ref{fig:modelDiagram} is presented here.

\subsection{Sexual reproduction} \label{subsec:crossingover}

An important consequence of the organization of the genome into chromosomes is the \textit{independent assortment} of chromosomes during sexual reproduction. That is, the variant of one chromosome inherited by progeny does not impact the inheritance of another. Within each chromosome, however, the process of \textit{crossing over} presents an additional mechanism of variability.

For clarity, consider briefly the process of meiosis. Recall that there exist two variants of each chromosome within every somatic\footnote{Somatic cells can be thought of as ``normal'' cells. Somatic cells are the cells which constitute an organism's body, in contrast to \textit{sex cells} or \textit{gametes} which are used for sexual reproduction.} cell, a paternally provided variant and a maternally provided variant. We introduce two new matrices to represent the maternal and paternal genomes of which $\m{G}$ is the offspring: $\m{M} = [\ve{m}_1| \ve{m}_2]$ and $\m{F} = [\ve{f}_1| \ve{f}_2]$ where $\ve{m}_1, \ve{m}_2, \ve{f}_1, \ve{f}_2 \in \mathcal{B}^{N_P}$. These represent the genomes of the mother and father of $\m{G}$ respectively. So, $\m{G}$ could be $[\ve{m}_1 | \ve{f}_2]$, for example. The particular method of inheritance in sexual reproduction corresponds to the construction of $\m{G}$ from one random column of $\m{M}$ and one random column of $\m{F}$. Hence, $\m{G}$ has one maternally provided variant and one paternally provided variant.

The mechanism is slightly more complex. During the production of sex cells used for sexual reproduction by meiosis, the columns of $\m{M}$ and $\m{F}$ may be perturbed. Rather than being inherited by $\m{G}$ in the same form as in $\m{M}$ and $\m{F}$, the physical process of producing sex cells can cause the swapping of column entries of $\m{M}$ and $\m{F}$ for large, contiguous sections of rows, and the resulting swapped columns are then inherited by $\m{G}$ as usual. These swaps are known as cross overs, and are the mechanism behind \textit{genetic recombination}.\footnote{Physically, this process is a result of the condensation of chromosomes of $\m{M}$ and $\m{F}$ into structures called \textit{chromatids} which pair along the centre of a meiosing cell. The resulting proximity creates the opportunity for these variants to physically cross over each other. Occasionally, this occurs in such a way that entire sections of the genome are switched between them. Without the genetic recombination resulting from this, $\m{G}$ would inherit one unmodified variant from each of $\m{M}$ and $\m{F}$, preventing totally new combinations of genetic material from occurring.}

\subsection{Modelling cross overs} \label{subsec:modelcrossing}

To model crossing over, begin by making the assumption that crossing over occurs independently for each chromosome, and will affect directly only that chromosome's variants.\footnote{Dependence between cross overs is known as \textit{interference}.} Consider a vector $\ve{h} \in \{1, \dots, C\}^{N_P}$ which denotes the chromosome of each row of $\m{M}$\footnote{Or, equivalently, $\m{F}$.}. For simplicity, assume that for all $i \leq j$, $h_i \leq h_j$, that is all base pairs of a chromosome appear in adjacent rows. As crossing over occurs independently for each chromosome, a crossing over event in chromosome $c$, say, will affect only those rows of $\m{M}$ where $\ve{h} = c$. For the moment, then, consider the case where $\ve{h}$ is a vector of ones, that is the case of a single chromosome.

Under this setting, let a cross over begin at the $i^{\text{th}}$ base pair. That is to say, suppose the chromosomes physically cross at the $i^{\text{th}}$ base pair.\footnote{We have an additional simplfying assumption here: that the chromosome will always be aligned such that the $i^{\text{th}}$ position on one variant will match with the $i^{\text{th}}$ on the other during a cross over.} Each variant is consquently separated into two parts: the part up to, but not including, the $i^{\text{th}}$ base pair, and the part from the $i^{\text{th}}$ base pair until the end. These two parts are then swapped between the variants, so that the first part of one variant forms a new chromosome with the second part of the other. Whenever the verb ``begin'' is used in the context of an index in a cross over, it will refer to this conceptualization, which corresponds to a swap of the values in the first $i-1$ rows of $\m{M}$. In order to track cross overs, introduce an indicator vector $\ve{v} = \tr{(v_1, \dots, v_{N_P})}$ where

\begin{equation} \label{eq:crossindicator}
V_i = \begin{cases}
  1 & \text{ if a cross over beginning at base pair } i \text{ occurs}, \\
  0 & \text{ otherwise},
\end{cases}
\end{equation}

\noindent and define $\sv{\pi}$ so that $\pi_i = P(V_i = 1)$. This can be done without loss of generality, as the order of crossing over events does not affect the final chromosome in this setting. Any chromosome in a sex cell for which a cross over has occurred is known as \textit{recombinant}.

As we rarely sequence the entire genome of an individual's somatic and sex cells, we will seldom see $\m{M}$ and its recombinant forms. Instead, $\m{S}$ is derived from $\m{G}$ and $\m{M}_S$ and $\m{F}_S$ from $\m{M}$ and $\m{F}$. From this, swaps of the markers of $\m{S}$, $\m{M}_S$, and $\m{F}_S$ between generations are used to estimate the number of sex cells containing recombinant chromosomes.\footnote{The proportion of sex cells produced with such a swap is called the \textit{recombination rate} for the pair of markers.} However, this method tells us nothing of how many cross over events occurred between any two markers. Any odd number of events leads to a swap, while any even number will be undetectable. Under this setting, the true count of indices $i$ for which $v_i = 1$ cannot be known, and hence the $\pi_i$ cannot be estimated individually.

\subsection{Simplifying Assumptions} \label{subsec:simplify}

Fortunately, if we only care about the recombination of two particular markers on the genome, estimating individual $\pi_i$ values is unnecessary. Consider two such positions, $j$ and $k$ with $j < k$, and note that cross overs beginning at $j+1, j+2, \dots, k-1, k$ all result in these positions being split between variants. Now, motivated by identifiability, assume that $\pi_j = \pi_{j+1} = \cdots = \pi_{k-1} = \pi_k = \pi_{j:k}$. Under this assumption, the number of cross overs beginning in $j+1,j+2,\dots,k-1,k$, is given by the binomial expression

$$P(N_c = n_c) = {k - j \choose n_c} \pi_{j:k}^{n_c} (1-\pi_{j:k})^{k - j - n_c},$$

\noindent where $N_c$ is a random variable giving the count of cross overs in the region. For convenience, let $r = k - j$ and $\pi = \pi_{j:k}$, which gives

$$P(N_c = n_c) = {r \choose n_c} \pi^{n_c} (1-\pi)^{r - n_c}.$$

\noindent Note here that $r$ is a unitless count of base pairs between positions $j$ and $k$. Recognizing that markers are often separated by a great number of base pairs, and so $r$ will typically be very large, we next take the limit of this expression as $r \rightarrow \infty$:

$$\lim_{r \rightarrow \infty} P(N_c = n_c) = \lim_{r \rightarrow \infty} {r \choose n_c} \pi^{n_c} (1-\pi)^{r - n_c}.$$

\noindent At this point, an arbitrary substitution can be made. Consider the substitution $\pi = \frac{\beta d(j,k)}{r} := \frac{\beta d}{r}$. By this substitution, the probability $\pi$ is reparameterized by a rate parameter, $\beta$, a distance measure, $d(j,k)$, and the $r$ base pairs separating $j$ and $k$. As the units of $\beta$ and $d$ will always result in a unitless product, the choices of $\beta$ and $d$ are a matter of individual discretion. Any convenient distance $d$ can be chosen and will invoke a corresponding $\beta$. If physical distance, for example in angstroms, were used, then $\beta$ would correspond to a rate of cross overs per unit length. One could alternatively use $d(j,k)=k-j$ to get a rate per base pair. As such a substitution is arbitrary, it gives a great deal of flexibility to choose a convenient set of units for measurement or understanding. Performing the substitution,

\begin{equation} \label{eq:poissonlim}
  \begin{split}
    \lim_{r \rightarrow \infty} P(N_c = n_c) & = \lim_{r \rightarrow \infty} \frac{r(r-1)\dots (r-n_c)}{n_c!} \left ( \frac{\beta d}{r} \right )^{n_c} \left ( 1-\frac{\beta d}{r} \right )^{r - n_c} \\
    & \\
    & = \lim_{r \rightarrow \infty} \frac{r^{n_c} + O(r^{n_c-1})}{n_c!} \left ( \frac{\beta d}{r} \right )^{n_c} \left ( 1-\frac{\beta d}{r} \right )^{r - n_c} \\
    & \\
    & = \lim_{r \rightarrow \infty} \frac{r^{n_c} + O(r^{n_c-1})}{r^{n_c}} \left ( \frac{(\beta d)^{n_c}}{n_c!} \right ) \left ( 1-\frac{\beta d}{r} \right )^{r - n_c} \\
    & \\
    & = \frac{(\beta d)^{n_c}}{n_c!} \lim_{r \rightarrow \infty} \frac{r^{n_c} + O(r^{n_c-1})}{r^{n_c}} \left ( 1-\frac{\beta d}{r} \right )^{r - n_c} \\
    & \\
    & = \frac{(\beta d)^{n_c}}{n_c!} e^{-\beta d}, \\
  \end{split}
\end{equation}

\noindent which is the Poisson limit theorem for the binomial distribution.

Recall that if $N_c$ is odd, it will result in a swap of markers $j$ and $k$ between variants, while if $N_c$ is even, there will be no resultant swap. Define the recombination probability $p_r(d)$, which gives the probability of observing a swap for positions $j$ and $k$ with distance $d(j,k) := d$ between them. Then $p_r(d)$ is given by a sum of all odd terms of the above distribution. Therefore,

\begin{equation} \label{eq:haldanemap}
  \begin{split}
    p_r(d) & = \sum_{l = 0}^{\infty} \frac{(\beta d)^{2l + 1}}{(2l + 1)!} e^{-\beta d} \\
    & \\
    & = e^{-\beta d} \sum_{l = 0}^{\infty} \frac{(\beta d)^{2l + 1}}{(2l + 1)!} \\
    & \\
    & = e^{-\beta d} \left ( \frac{e^{\beta d} - e^{- \beta d}}{2} \right ) \\
    & \\
    & = \frac{1}{2} \left ( 1 - e^{-2 \beta d} \right ) .\\
  \end{split}
\end{equation}

\noindent A final substitution converts Equation \ref{eq:haldanemap} to a form familiar to researchers in genomics. Setting $\beta = \frac{1}{100}$ so that each each unit increase in $d$ corresponds to a 0.01 increase in the expected number of crossing over events gives us Haldane's formula for the \textit{map distance} in \textit{centiMorgans}. These distance units are defined purely by the assumptions leading to Equation \ref{eq:poissonlim} and the relationship specified in Equation \ref{eq:haldanemap} with the choice of $\beta = \frac{1}{100}$, rather than reflecting a true physical distance.\footnote{One may imagine this distance measure was chosen out of pure convenience. J.B.S. Haldane devised the approximation in 1919, at which point the chemical structure of DNA was still a mystery, but the concept of recombination had been revealed through patterns of inheritance.}

\section{Genetic correlation} \label{sec:correlation}

Recall $\ve{z}$ as depicted in Figure \ref{fig:modelDiagram} and described in the beginning of Section \ref{sec:intro}. Typically, when the notion of the correlation between markers is discussed in a GWAS, what is actually under discussion is the observed correlation matrix of this vector in a particular population (e.g. \cite{cheverud2001}). As a consequence, this quantity is key in the determination of the effective number of tests in GWASs. Despite the prevalence of simulation to compute this matrix in the literature, it can also be determined analytically.

For clarity, let $\ve{z}$ indicate an instance of the random vector $\ve{Z} = \tr{(Z_1, Z_2, \dots, Z_M)}$. We let the random vector $\ve{Z}$ follow the distribution of the summarized values $\ve{z}$ in a particular population. This population may be real, as is the case when this modelling is used in practice, or purely hypothetical, as will be the case in the following analysis.

Return to the annotated matrix $\m{T}$ and consider two markers at row indices $j$ and $k$. Introduce $\ve{c}$, which is defined similarly to $\ve{h}$ earlier, but now indicates chromosomal membership for the markers in $\m{T}$ rather than the base pairs in $\m{G}$. As every marker is contained within a single chromosome, $\ve{c}$ is always unambiguously defined.

There are two cases. Either $j$ and $k$ are on the same chromosome, that is $h_j = h_k$, or they are not, and so $h_j \neq h_k$. If these markers are not on the same chromosome, we can use the assumptions of Section \ref{subsec:modelcrossing} to see immediately that there will be no correlation between $Z_j$ and $Z_k$, as these markers will assort independently with their different chromosomes. If they are on the same chromosome, let $d(j,k) = d$ be the distance between them measured in centiMorgans. Denote the dominant and recessive alleles with $A$ and $a$ respectively for $j$ and use $B$ and $b$ analogously for $k$. Assume that the pairwise association of these markers in the population is of interest, i.e. that we can ignore all other markers on this chromosome in our analysis. Under this setting, we may consider a radically simplified $\m{T}$, with 2 rows rather than $M$ and taking the form

$$\m{T} = \begin{bmatrix}
  A & a \\
  b & B \\
\end{bmatrix},$$

\noindent where the letters placed above are merely demonstrative. A simplified version of $\m{X}$ follows immediately from this $\m{T}$. Consider

$$\m{X} = \begin{bmatrix}
  x_{j1} & x_{j2} \\
  x_{k1} & x_{k2} \\
\end{bmatrix},$$

\noindent with all entries in $\{0,1\}$. As was the case for $\ve{z}$, we can treat these lowercase entries as realizations of random variables $X_{rs}$, $r \in \{j,k\}, s \in \{1,2\}$.\footnote{No additional notation is used to denote whether $\m{X}$ is the random matrix or a realization, as this distinction is not relevant in the analysis presented here.} Using this form of $\m{X}$, we can consider $Cor(Z_j, Z_k)$ for the population resulting from an arbitrary cross of two parents. Then $\m{X}$ implies a $\ve{Z}$ of
$$\ve{Z} = \begin{bmatrix} Z_j \\ Z_k \end{bmatrix} = \begin{bmatrix}
  X_{j1} + X_{j2} \\
  X_{k1} + X_{k2} \\
\end{bmatrix}.$$
The mechanics of sexual reproduction outlined in Section \ref{subsec:crossingover} and the genotype of the parents crossed to create $\m{X}$ determine the distribution of $Z_j$ and $Z_k$. Recall $\m{M}$ and $\m{F}$ introduced alongside sexual reproduction. Introduce simplified, annotated forms of these matrices here to represent the paternal and maternal encodings
$$\m{F}_X = \begin{bmatrix}
  f_{j1} & f_{j2} \\
  f_{k1} & f_{k2} \\
\end{bmatrix}, \text{ and }
\m{M}_X = \begin{bmatrix}
  m_{j1} & m_{j2} \\
  m_{k1} & m_{k2} \\
\end{bmatrix},$$
where all entries are once again in $\{0,1\}$. Begin by assuming that $\m{F}_X$ and $\m{M}_X$ are known constants\footnote{There are many theoretical populatins where this is true, such as the $F_2$ intercross, where $f_{11} = m_{11} = f_{21} = m_{21} = 1$ and $f_{12} = m_{12} = f_{22} = m_{22} = 0$.}, and the statistical properties of $\ve{Z}$ result purely from the mechanics of reproduction.

Begin by considering the expectation of $\ve{Z}$. Due to the independent assortment of chromosomes, $X_{j1}$ is equally likely to be either $f_{j1}$ or $f_{j2}$, and so takes a uniform distribution over these two possibilities. A similar logic for all other entries in $\m{X}$ applies, and so
\begin{equation*}
  \begin{split}
    E[\ve{Z}] & = {\begin{bmatrix}
        E[X_{j1}] + E[X_{j2}] \\
        E[X_{k1}] + E[X_{k2}] \\
      \end{bmatrix}} \\
    & \\
    & = {\frac{1}{2}\begin{bmatrix}
        f_{j1} + f_{j2} + m_{j1} + m_{j2} \\
        f_{k1} + f_{k2} + m_{k1} + m_{k2} \\
      \end{bmatrix}}, \\
  \end{split}
\end{equation*}
from which it follows
\begin{equation*}
  \begin{split}
    Var(Z_j) & = E[(X_{j1} + X_{j2})^2] - E[Z_j]^2 \\
    & = \frac{1}{4}\left [ (f_{j1} + m_{j1})^2 + (f_{j2} + m_{j1})^2 + (f_{j1} + m_{j2})^2 + (f_{j2} + m_{j2})^2 \right ] \\
    & \hspace{0.5cm} - \frac{1}{4}(f_{j1} + f_{j2} + m_{j1} + m_{j2})^2. \\
  \end{split}
\end{equation*}
This can be simplified to give
\begin{equation} \label{eq:z1var}
  Var(Z_j) = \frac{1}{4} \left [ (f_{j1} - f_{j2})^2 + (m_{j1} - m_{j2})^2 \right ].
\end{equation}
Analogously,
\begin{equation} \label{eq:z2var}
  Var(Z_k) = \frac{1}{4} \left [ (f_{k1} - f_{k2})^2 + (m_{k1} - m_{k2})^2 \right ].
\end{equation}
Considering the covariance:
\begin{equation} \label{eq:covstep1}
  \begin{split}
    Cov(Z_j, Z_k) & = Cov(X_{j1} + X_{j2}, X_{k1} + X_{k2}) \\
    & \\
    & =  Cov(X_{j1}, X_{k1}) + Cov(X_{j1}, X_{k2}) + Cov(X_{j2}, X_{k1}) + Cov(X_{j2}, X_{k2}). \\
  \end{split}
\end{equation}
So the covariance is re-expressed as a sum of four terms, each of which can then be considered in turn.

This can be further simplified by considering $Cov(X_{j1}, X_{k2})$ and $Cov(X_{j2}, X_{k1})$. Both of these terms measure the covariance between values on the diagonals of $\m{X}$, that is the covariance between the maternally and paternally donated variants of the genome inherited from $\m{F}_X$ and $\m{M}_X$, respectively. These covariances therefore measure the degree of \emph{inbreeding} in a population, the degree to which parents tend to have the same genotype. The restricted setting of known parents derived here makes these diagonal values independent of each other and therefore uncorrelated, which can be confirmed by tedious algebra. Explicitly, $Cov(X_{j1}, X_{k2}) = Cov(X_{j2}, X_{k1}) = 0$. In unrestricted settings \cite{crowkimura1970intro} quantify this with the coefficient $r$ under \emph{assortative mating}, the tendency of individuals to choose mates with a similar phenotype.  

The second pair of terms, $Cov(X_{j1}, X_{k1})$ and $Cov(X_{j2}, X_{k2})$, measure the covariance of encodings on the same variant, and so cannot be so easily reduced. Instead, consider $Cov(X_{j1}, X_{k1})$ and expand:
$$Cov(X_{j1}, X_{k1}) = E[X_{j1} X_{k1}] - E[X_{j1}]E[X_{k1}].$$
The independent inheritance of variants from a parent gives $E[X_{j1}] = \frac{1}{2}(f_{j1} + f_{j2})$ and $E[X_{k1}] = \frac{1}{2}(f_{k1} + f_{k2})$. Next consider $E[X_{j1} X_{k1}]$.

There are four possible values of $X_{j1} X_{k1}$, corresponding to inheritance of either of the two parental variants with or without recombination. If no recombination occurs, an event with probability $1 - p_r(d)$, either $f_{j1} f_{k1}$ or $f_{j2} f_{k2}$ is inherited with equal probability. If a cross over between $j$ and $k$ leads to recombination, then either $f_{j1} f_{k2}$ or $f_{j2} f_{k1}$ is passed on with equal probability. Accounting for these four possibilities gives
$$E[X_{j1} X_{k1}] = (1 - p_r(d)) \left ( \frac{1}{2} f_{j1} f_{k1} + \frac{1}{2} f_{j2} f_{k2} \right ) + p_r(d) \left ( \frac{1}{2} f_{j1} f_{k2} + \frac{1}{2} f_{j2} f_{k1} \right ).$$
Combining this with the expectations of $X_{j1}$ and $X_{k1}$ gives
\begin{equation} \label{eq:covstep2}
  \begin{split}
    Cov(X_{j1}, X_{k1}) & = E[X_{j1} X_{k1}] - E[X_{j1}]E[X_{k1}] \\
    & \\
    & = (1 - p_r(d)) \left ( \frac{1}{2} f_{j1} f_{k1} + \frac{1}{2} f_{j2} f_{k2} \right ) + p_r(d) \left ( \frac{1}{2} f_{j2} f_{k1} + \frac{1}{2} f_{j1} f_{k2} \right ) \\
    & \hspace{0.5cm} - \frac{1}{4} (f_{j1} + f_{j2})(f_{k1} + f_{k2}) \\
    & \\
    & = \frac{1}{4} \left ( 1 - 2 p_r(d) \right ) (f_{j1}f_{k1} + f_{j2}f_{k2} - f_{j2}f_{k1} - f_{j1}f_{k2}) \\
    & \\
    & = \frac{1}{4} \left ( 1 - 2 p_r(d) \right ) (f_{j1} - f_{j2})(f_{k1} - f_{k2}) .\\
  \end{split}
\end{equation}
The same logic can be applied to $Cov(X_{j2}, X_{k2})$ to obtain
\begin{equation} \label{eq:covstep3}
  Cov(X_{j2}, X_{k2}) = \frac{1}{4} \left ( 1 - 2 p_r(d) \right ) (m_{j1} - m_{j2})(m_{k1} - m_{k2}).
\end{equation}
We obtain the covariance of $Z_j$ and $Z_k$ by adding the above and Equation \ref{eq:covstep1}. Substituting Equations \ref{eq:covstep2} and \ref{eq:covstep3} and $Cov(X_{j1}, X_{k2}) = Cov(X_{j2}, X_{k1}) = 0$ gives
\begin{equation} \label{eq:cov}
  Cov(Z_j, Z_k) = \frac{1}{4} (1 - 2 p_r(d)) \left [ (f_{j1} - f_{j2})(f_{k1} - f_{k2}) + (m_{j1} - m_{j2})(m_{k1} - m_{k2}) \right ].
\end{equation}
Finally, Equations \ref{eq:z1var}, \ref{eq:z2var}, and \ref{eq:cov} can be combined to determine the correlation:
\begin{equation} \label{eq:precorr}
  \begin{split}
    Corr(Z_j, Z_k) & = \frac{Cov(Z_j, Z_k)}{\sqrt{Var(Z_j) Var(Z_k)}}\\
    & \\
    & = \frac{ \frac{1}{4} (1 - 2 p_r(d)) \left [ (f_{j1} - f_{j2})(f_{k1} - f_{k2}) + (m_{j1} - m_{j2})(m_{k1} - m_{k2}) \right ] }{ \frac{1}{4} \sqrt{ \left [ (f_{j1} - f_{j2})^2 + (m_{j1} - m_{j2})^2 \right ] \left [ (f_{k1} - f_{k2})^2 + (m_{k1} - m_{k2})^2 \right ] }} \\
    & \\
    & = (1 - 2 p_r(d)) \frac{ (f_{j1} - f_{j2})(f_{k1} - f_{k2}) + (m_{j1} - m_{j2})(m_{k1} - m_{k2}) }{ \sqrt{ \left [ (f_{j1} - f_{j2})^2 + (m_{j1} - m_{j2})^2 \right ] \left [ (f_{k1} - f_{k2})^2 + (m_{k1} - m_{k2})^2 \right ] }} \\
    & \\
    & := (1 - 2 p_r(d)) \gamma .\\
  \end{split}
\end{equation}
So, the correlation is seen to be a product of $(1-2 p_r(d))$, which depends on the markers in question, and a factor $\gamma$, which depends on the parents being crossed. An even simpler expression is obtained by substituting the Haldane recombination probability from Equation \ref{eq:haldanemap} in place of $p_r(d)$:

\begin{equation} \label{eq:corrdist}
  \begin{split}
    Corr(Z_j, Z_k) & = (1 - 2 p_r(d)) \gamma \\
    & \\
    & = \left ( 1 - 2 \left [ \frac{1}{2} \left ( 1 - e^{-2 \beta d} \right ) \right ] \right ) \gamma \\
    & \\
    & = \gamma e^{-2 \beta d}, \\
  \end{split}
\end{equation}
and so the using the Haldane map distance the correlation between $Z_j$ and $Z_k$ decays exponentially in $d(1,2)$ with a constant $\gamma$ determined by the parents being crossed.

As $f_{rs}, m_{rs} \in \{0, 1\}$, the differences used to determine $\gamma$ are all either -1, 0, or 1. There are therefore $3^4 = 81$ potential $\gamma$ values, those most of these are not unique.

%However, the constraint that $f_{ij}, m_{ij} \in \{0, 1\}$ means that $\gamma$ can be rewritten slightly. Recognizing that the pairwise differences in Equation \ref{eq:precorr} are equivalent to indicators of inequality between terms. That is, $f_{11} - f_{12} = I_{f_{11} \neq f_{12}}$ and similarly for the other terms. $\gamma$ can be expressed as

%$$\gamma =  \frac{ I_{f_{11} \neq f_{12}} I_{f_{21} \neq f_{22}} + I_{m_{11} \neq m_{12}}I_{m_{21} \neq m_{22}} }{ \sqrt{ \left ( I_{f_{11} \neq f_{12}} + I_{m_{11} \neq m_{12}} \right ) \left ( I_{f_{21} \neq f_{22}} + I_{m_{21} \neq m_{22}} \right ) }},$$

%\noindent which, while not much easier to write, is conceptually clearer. The binary nature of each $f_{ij}$ and $m_{ij}$ leaves only 16 distinct crosses, and so 16 values of $\gamma$. These are enumerated in in Table \ref{tab:gamma}.

%\begin{table}[!ht]
%  \centering
%  \caption{An enumeration of the possible values of the indicators and $\gamma$.}
%  \label{tab:gamma}
%  \begin{tabular}{|c c c c|c|} \hline
%    $I_{f_{11} \neq f_{12}}$ & $I_{f_{21} \neq f_{22}}$ & $I_{m_{11} \neq m_{12}}$ & $I_{m_{21} \neq m_{22}}$ & $\gamma$ \\ \hline
%    0 & 0 & 0 & 0 & 0 \\ \hline
%    0 & 0 & 0 & 1 & 0 \\ \hline 
%    0 & 0 & 1 & 0 & 0 \\ \hline
%    0 & 0 & 1 & 1 & 1 \\ \hline
%    0 & 1 & 0 & 0 & 0 \\ \hline
%    0 & 1 & 0 & 1 & 0 \\ \hline
%    0 & 1 & 1 & 0 & 0 \\ \hline
%    0 & 1 & 1 & 1 & $1 / \sqrt{2}$ \\ \hline
%    1 & 0 & 0 & 0 & 0 \\ \hline
%    1 & 0 & 0 & 1 & 0 \\ \hline
%    1 & 0 & 1 & 0 & 0 \\ \hline
%    1 & 0 & 1 & 1 & $1 / \sqrt{2}$ \\ \hline
%    1 & 1 & 0 & 0 & 1 \\ \hline
%    1 & 1 & 0 & 1 & $1 / \sqrt{2}$ \\ \hline
%    1 & 1 & 1 & 0 & $1 / \sqrt{2}$ \\ \hline
%    1 & 1 & 1 & 1 & 1 \\ \hline
%  \end{tabular}
%\end{table}   

%Surprisingly, many of the possible crosses admit a $\gamma$ value of zero, suggesting that for a slight majority of possible population designs, markers will be uncorrelated. A second critical observation is that, for the non-zero correlation populations, the structure is dictated by $1 - 2p_r(d)$, which may be modified by a factor of $\frac{1}{\sqrt{2}}$. This implies that the only relevant process for modelling correlation in this model is the probability of cross overs between given markers.

Recalling that $j$ and $k$ were restricted to be markers on the same chromosome, this pairwise result can be immediately generalized to the correlation matrix $\ve{Z}$ for the markers measured on an entire genome. For markers on the same chromosome, non-trivial correlations will be proportional to $1 - 2p_r(d)$ where $p_r(d)$ indicates the probability of recombination as a function of the distance between markers. Based on the independent assortment of different chromosomes, the correlations will be zero for any pair $j$ and $k$ not on the same chromosome.

In other words, if $h_j = h_k$, Equation \ref{eq:precorr} dictates the correlation between $Z_j$ and $Z_k$. On the other hand, if $h_j \neq h_k$ the correlation between $Z_j$ and $Z_k$ will be zero. This implies a block diagonal structure corresponding to the chromosomes with correlations dictated by the probability of recombination within each chromosome. Most generally
\begin{equation} \label{eq:zcorr_gen}
  Corr(Z_j, Z_k) = \ind{h_k}{\{h_j\}} \gamma (1 - 2p_r(d)),
\end{equation}
and under the Haldane model Equation \ref{eq:corrdist} gives
\begin{equation} \label{eq:zcorr}
  Corr(Z_j, Z_k) = \ind{h_k}{\{h_j\}} \gamma e^{-2 \beta d(j,k)}.
\end{equation}

\noindent There are two particular population structures for which $\gamma$ is of interest, due to their popularity.\footnote{The populations of these crosses are entirely hypothetical in humans, but through successive inbreeding over many generations such homogeneity is commonly created in mouse models.}

First, consider the \textit{$F_2$ intercross} design. This design considers the population resulting from the cross of $\m{M}_X$ and $\m{F}_X$ with

$$\m{F}_X = \m{M}_X = \begin{bmatrix}
  1 & 0 \\
  1 & 0 \\
\end{bmatrix}$$

\noindent and so corresponds to the setting where all the differences in $\gamma$ are 1. Therefore, $\gamma_{\text{inter}} = 1$ and so the correlation for an intercross population is given by $Corr(Z_j, Z_K) = I_{h_j = h_k} e^{-2 \beta d(j,k)}$.

For the \textit{$F_2$ backcross}, a slightly different setting is used. Here we have a cross between $\m{M}_X$ and $\m{F}_X$ defined as

$$\m{F}_X = \begin{bmatrix}
  f & f \\
  f & f \\
\end{bmatrix}, \text{ and }
\m{M}_X = \begin{bmatrix}
  1 & 0 \\
  1 & 0 \\
\end{bmatrix}.$$

\noindent In this setting, both differences defined on $\m{F}_X$ are 0 while both of those defined on $\m{M}_X$ are 1. This immediately gives $\gamma_{\text{back}} = 1$, and so the correlation for the backcross population is given by the exact same expression as that of the intercross population, $Corr(Z_j, Z_K) = I_{h_j = h_k} e^{-2 \beta d(j,k)}$.\footnote{As an aside, note that names \textit{intercross} and \textit{backcross} are references to breeding practices. The $\m{F}$ and $\m{M}$ of the intercross can be thought of as the result of a cross between an individual which is entirely dominant and an individual which is entirely recessive. The in the intercross, two children resulting from this dominant/recessive cross are crossed, while in the backcross, a child is crossed with a parent. Hopefully, one appreciates why such odd patterns of breeding are only pursued in mice.}

Other interesting cases involve
$$\m{F}_X = \begin{bmatrix}
  0 & 1 \\
  1 & 0 \\
\end{bmatrix} \text{ or } \m{M}_X = \begin{bmatrix}
  0 & 1 \\
  1 & 0 \\
\end{bmatrix},$$
as these can result in $\gamma < 0$, and so a negative correlation. For example, if we have
$$\m{F}_X = \m{M}_X = \begin{bmatrix}
  0 & 1 \\
  1 & 0 \\
\end{bmatrix},$$
then $\gamma = -1$, while taking
$$\m{F}_X = \begin{bmatrix}
  0 & 1 \\
  1 & 0 \\
\end{bmatrix} \text{ and } \m{M}_X = \begin{bmatrix}
  0 & 1 \\
  0 & 1 \\
\end{bmatrix},$$
gives $\gamma = -\frac{1}{\sqrt{2}}$.

Many other settings lead to no measured correlation. Take
$$\m{F}_X = \begin{bmatrix}
  0 & 1 \\
  1 & 0 \\
\end{bmatrix} \text{ and } \m{M}_X = \begin{bmatrix}
  1 & 1 \\
  0 & 0 \\
\end{bmatrix},$$
or
$$\m{F}_X = \begin{bmatrix}
  0 & 1 \\
  1 & 0 \\
\end{bmatrix} \text{ and } \m{M}_X = \begin{bmatrix}
  1 & 1 \\
  0 & 0 \\
\end{bmatrix},$$

\section{Simulating the model} \label{subsec:sim}

The model outlined in Figure \ref{fig:modelDiagram} also suggests a straightforward set of structures which can be used for simulation. Motivated by the focus on correlation in determining $M_{eff}$, these simulations focused on simulating the correlation of $\ve{z}$ for both the $F_2$ intercross and $F_2$ backcross populations.

For both populations, 100,000 individuals were simulated under three separate settings. In all settings, the simulated genomes consisted of 20 markers with differing structure. In the first setting, call it setting (a), the simplest possible case was examined: that of markers spaced evenly along a single chromosome. A distance of distance of 15 cM between adjacent markers was chosen for this setting. Setting (b) also investigated markers on a single chromosome, but these markers were no longer equidistant. Instead, markers one through six were separated by adjacent distances of 2 cM. 6 through 11 by adjacent distances of 5 cM, and similarly 11 through 16 were separated by 10 cM and 16 through 20 by 20 cM. Finally, in setting (c) a genome of two chromosomes was investigated, with the markers on the first placed 5 cM apart and those on the second 15 cM apart. The resulting correlation matrices for these settings are displayed in Figure \ref{fig:backsim} for the $F_2$ backcross population and Figure \ref{fig:intersim} for the $F_2$ intercross population.

\begin{figure}[htp]
  \begin{center}
    \begin{tabular}{ccc}
      \includegraphics[width = 0.300\textwidth]{./img/back1.png} &
      \includegraphics[width = 0.300\textwidth]{./img/back2.png} &
      \includegraphics[width = 0.300\textwidth]{./img/back3.png} \\
      {\footnotesize (a) Single chromosome,} &
      {\footnotesize (b) Single chromosome,} &
      {\footnotesize (c) Two chromosomes,} \\
      {\footnotesize markers 15 cM apart} &
      {\footnotesize markers 2, 5, 10, } &
      {\footnotesize markers 5 and 15 cM} \\
      &
      {\footnotesize and 20 cM apart} &
      {\footnotesize apart} \\ 
    \end{tabular}
  \end{center}
  \caption{$F_2$ backcross correlation matrices for the three settings simulated.}
  \label{fig:backsim}
\end{figure}

\begin{figure}[htp]
  \begin{center}
    \begin{tabular}{ccc}
      \includegraphics[width = 0.300\textwidth]{./img/inter1.png} &
      \includegraphics[width = 0.300\textwidth]{./img/inter2.png} &
      \includegraphics[width = 0.300\textwidth]{./img/inter3.png} \\
      {\footnotesize (a) Single chromosome,} &
      {\footnotesize (b) Single chromosome,} &
      {\footnotesize (c) Two chromosomes,} \\
      {\footnotesize markers 15 cM apart} &
      {\footnotesize markers 2, 5, 10, } &
      {\footnotesize markers 5 and 15 cM} \\
      &
      {\footnotesize and 20 cM apart} &
      {\footnotesize apart} \\ 
    \end{tabular}
  \end{center}
  \caption{$F_2$ intercross correlation matrices for the three settings simulated.}
  \label{fig:intersim}
\end{figure}

The patterns present in Figures \ref{fig:backsim} and \ref{fig:intersim} are exactly as expected from Section \ref{sec:correlation}. There is an almost perfect similarity between the backcross and intercross design, with only small local variations present between them, in particular for setting (a). All settings show a clear maximum along the main diagonal and a regular decay for off-diagonal elements. Indeed, setting (b) demonstrates how this decay is faster for more distantly spaced markers than nearer ones with its distinct fan pattern. Setting (c) additionally demonstrates the lack of correlation between different chromosomes with its striking block structure for both the backcross and the intercross.

Such simulations are a confirmation of the earlier analysis performed, but were much more time consuming than using the analytical results. This suggests that the current standard method of simulating large populations is a rather inefficient way of achieving the same result as the analytical prescription of Section \ref{sec:correlation}. Conveniently, the prescription ignores the particular setting: both the backcross and intercross behave identically when viewed through the lens of correlation.

\subsection{Implementation}

\TODO{Lose this section?}

In order to make the above procedures repeatable, easily read, and flexible, the code for these simulations was implemented in \R using the S3 class system. The core construct was a class meant to reflect $\m{X}$.

Objects of the \code{genome} class are lists with two elements: \code{alleles} and \code{dists}. The \code{alleles} element is a list of two column matrices, where each matrix represents the value of $\m{X}$ for a particular chromosome. \code{dists} is a list of vectors of the same length as \code{alleles}. Each vector in \code{dists} gives the distances between the alleles in the corresponding element of \code{alleles}. A function called \code{abiogenesis} allows for the convenient creation of a genome through the specification of distances and allele values.

The function \code{sex} is then used to cross any pair of \code{genome} objects with the use of a \code{meiosis} helper function. \code{sex} accepts an arbitrary distance function which is passed to \code{meiosis} to convert the \code{dists} elements of the \code{genome}s into probabilities of crossing over. By default, the Haldane map distance conversion of Equation \ref{eq:haldanemap} is used. Random Bernoulli trials for each of these probabilities then determines the locations of cross over events, and sections of the columns of \code{alleles} are swapped accordingly.

The correlation of repeated swaps is then computed via \code{popCorrelation}. As the choice of scoring is at the discretion of the analyst, it accepts not only a list of \code{genome} objects representing the result of repeated crosses, but also a scoring function which accepts a single \code{genome} and returns a $\ve{z}$ value. By default, the additive scoring of $\ve{z} = \ve{x}_1 + \ve{x}_2$ is used.

The score function generates the $\ve{z}$ values for each \code{genome} provided to \code{popCorrelation}, and correlations between the elements of these $\ve{z}$ are computed. Finally, the \code{image} wrapper \code{corrImg} displays a correlation matrix rearranged so that the main diagonal is consistent with the typical arrangement of correlation matrices. The code for this implementation can be found at https://github.com/Salahub/genetic-model.

\section{Comparing the model to reality} \label{sec:model2real}

Though simulation confirms that population correlations generated under the model of sexual reproduction described in \ref{subsec:crossingover} match the predictions of Equation \ref{eq:zcorr}, this does not mean it reflects genetic mechanics with fidelity. Evaluating the extent to which this is the case requires data.

Luckily, \cite{cheverud2001} cites earlier work by \cite{cheverudetal2001} in which the two pure mouse strains were used to generate an $F_2$ intercross population. \cite{cheverud2001} reproduces a correlation matrix of $\ve{z}$ under the additive mapping for this population, providing the opportunity to compare Equation \ref{eq:zcorr} to observed correlations generated under the same setting. Figure \ref{fig:corr2real} displays the results of this comparison.

\begin{figure}[htp]
  \begin{center}
    \begin{tabular}{cc}
      \includegraphics[width = 0.300\textwidth]{./img/chevCorr.png} &
      \includegraphics[width = 0.300\textwidth]{./img/chevCorrTheory.png} \\
      {\footnotesize Experiment} &
      {\footnotesize Theory}
    \end{tabular}
  \end{center}
  \caption{Experimental and theoretical correlation matrices for an $F_2$ intercross population.}
  \label{fig:corr2real}
\end{figure}

The theoretical and experimental matrices look rather similar structurally, with similar trends and local patterns. Taking the difference suggests that theoretical correlation tends to overestimate the actual correlation between $z_j$ and $z_k$. In a few cases this underestimation is rather severe. \TODO{Is it severe? Think about this shading more carefully: scaled, CI, p-val, or the like}

\begin{figure}[htp]
  \begin{center}
      \includegraphics[width = 0.7\textwidth]{./img/chevCorrTest.png}
  \end{center}
  \caption{The correlation test plot for data from \cite{cheverudetal2001} compared to simulations under theory.}
  \label{fig:corrTestPlot}
\end{figure}

That said, it performs reasonably well, especially given that \cite{haldane1919} proposed the distance measure at its core more than a century ago.

\begin{figure}[htp]
  \begin{center}
    \begin{tabular}{cc}
      \includegraphics[width = 0.300\textwidth]{./img/jaxbsb.png} &
      \includegraphics[width = 0.300\textwidth]{./img/jaxbsb_sim.png} \\
      {\footnotesize Observed} &
      {\footnotesize Simulated}
    \end{tabular}
  \end{center}
  \caption{Observed and simulated correlations for the JAX BSB panel from the Mouse Genome Database.}
  \label{fig:jaxbsb}
\end{figure}


\begin{figure}[htp]
  \begin{center}
    \begin{tabular}{cc}
      \includegraphics[width = 0.300\textwidth]{./img/uclabsb.png} &
      \includegraphics[width = 0.300\textwidth]{./img/uclabsb_sim.png} \\
      {\footnotesize Observed} &
      {\footnotesize Simulated}
    \end{tabular}
  \end{center}
  \caption{Observed and simulated correlations for the UCLA BSB panel from the Mouse Genome Database.}
  \label{fig:uclabsb}
\end{figure}


\section{Other views} \label{sec:apply}

While the development of the model in Section \ref{sec:theModel} focused on heredity from a given father and mother, a radically simplified model can be considered for a population of unknown heritage. Rather than considering the values in $\m{X}$ for an individual in this population in terms of the parental variants, the possible values can, instead, be considered directly when determining the pairwise relationship between two markers.

Recall the simplified two marker versions of $\m{X}$ from Section \ref{sec:correlation}. Consider a column of $\m{X}$, say $\ve{x}_1$, and note that the two entries of $\ve{x}_1$ only take values in $\{0,1\}$, giving four possible combinations. For every possible combination, there is a corresponding probability of an individual in a population having such a combination on a variant. Suppose these are defined as in Table \ref{tab:r2}.

\begin{table}[!ht]
  \centering
  \begin{tabular}{c c| c c| c}
    & \multicolumn{1}{c}{} & \multicolumn{2}{c}{$x_{11}$} & \\ \cline{3-4}
    & & 0 & 1 & \underline{Marginal} \\ \cline{2-4}
    \multirow{2}{*}{$x_{21}$} & \multicolumn{1}{|c|}{0} & $p_{ab}$ & $p_{Ab}$ & $1-p_B$ \\
    & \multicolumn{1}{|c|}{1} & $p_{aB}$ & $p_{AB}$ & $p_B$ \\ \cline{2-4}
    & \multicolumn{1}{c}{Marginal} & \multicolumn{1}{c}{$1 - p_A$} & \multicolumn{1}{c}{$p_A$} &  \\ 
  \end{tabular}
  \caption{Probabilities of combinations of $\ve{x}_1$ in a population of individuals.}
  \label{tab:r2}
\end{table}

This conceptualization, which does not directly consider the parentage of a population, is dominant in the literature.

\subsection{Settings in the Literature} \label{subsec:inlit}

Despite this difference, the settings introduced in Section \ref{subsec:sim} occur commonly in the literature. Among studies focusing on real data, including \cite{Galwey2009}, \cite{nyholt2004}, and \cite{Salyakina2005}, the structure of selected markers is motivated by previous work. As a consequence, these studies typically only view one chromosome, indeed one small section of a chromosome, associated with a phenotype. Additionally, the markers within this segment are typically not evenly spaced, a situation similar to setting (b) from Section \ref{subsec:sim}. Typically, these distances are not reported in centimorgans, but instead in base pairs.

In contrast, simulation studies seem to be characterized by equidistant measurements on one or several chromosomes. \cite{cheverud2001} performs simulation on a single chromosome with equidistant markers, and records the results for several distances. This is of the same form as setting (a) from Section \ref{subsec:sim}. \cite{LanderBotstein1989} examines a case of 12 chromosomes with markers spaced every 20 cM along each, which is a specific case of setting (c).

Departing from a reference to distances in centimorgans or base pairs, \cite{LiJi2005} set their simulation scenarios using the genetic $r^2$ measure, defined by \cite{hillrobertson1968} as

\begin{equation} \label{eq:rsq}
  r^2 = \frac{\left ( p_{AB} p_{ab} - p_{Ab} p_{aB} \right )^2}{{p_A (1 - p_A) p_B (1 - p_B)}} = \frac{\left ( p_{AB} - p_A p_B \right )^2}{p_A (1 - p_A) p_B (1 - p_B)},
\end{equation}

\noindent which is exactly Pearson's product moment correlation for the two by two contingency table case. In adopting this measure to define their simulation settings, \cite{LiJi2005} use different language than other studies. Their simulation is described as an investigation of ten independent regions within which five markers are placed such that adjacent markers have an $r^2$ of 0.8 between them. Despite this difference in language, this design is clearly analogous to setting (c) from Section \ref{subsec:sim}.

The use of $r^2$ by \cite{LiJi2005} to specify their population parameters is somewhat curious, however. $r^2$ presents difficulties for measuring genetic distance over generations in any model including recombination. Consider $p_{AB}$, $p_A$, and $p_B$ and their relationship over generations. Without selection in survival or mating, $p_A$ and $p_B$ will remain constant, while $p_{AB}$ will change.

Suppose we have an offspring with
$$\m{G} = \begin{bmatrix}
  1 & g_{12} \\
  1 & g_{22} \\
\end{bmatrix}.$$
There are two possibilities for $\m{M}$ which could result in this particular $\m{G}$ when the independence of variant heritability is considered:
$$\m{M} = \begin{bmatrix}
  1 & g_{12} \\
  1 & g_{22} \\
\end{bmatrix}, \text{ or }
\m{M} = \begin{bmatrix}
  g_{11} & 1 \\
  1 & g_{22} \\
\end{bmatrix}.$$
In the first of these two $\m{M}$ configurations, $\m{G}$ results if no recombination occurs, while in the second $\m{G}$ results if recombination occurs. The first configuration occurs with probability $p_{AB}$ and is passed on with probability $1 - p_r(d)$, as recombination disturbs the necessary variant. The second configuration occurs with probability $p_Ap_B$ and is passed on with probability $p_r(d)$.

Denoting $p_{AB,k}$ as the value of $p_{AB}$ at generation $k$, we then write

$$p_{AB,k} = [1 - p_r(d)] p_{AB,k-1} + p_r(d) p_A p_B,$$

\noindent from which it can easily be derived that

$$r^2_k = \left [ 1 - p_r(d) \right ]^2 r^2_{k-1} =  \left [ 1 - p_r(d) \right ]^{2k} r^2_0$$

\noindent and so

$$\lim_{k \rightarrow \infty} \left [ 1 - p_r(d) \right ]^{2k} r^2_0 = 0$$

\noindent whenever $p_r(d) > 0$.

In this manner, strongly associated markers $A$ and $B$ become less associated over time in the absence of selection pressures in sexual reproduction and survival, as noted in \cite{siegmundyakir2007}. This makes the use of $r^2$ under any model with recombination problematic, as its use requires the specification of an initial condition and generation of interest. \cite{LiJi2005} simulate data using given $p_A$, $p_B$, and $r^2$ values to determine $p_{AB}$ and generate a population which matches $p_A$, $p_B$, and $p_{AB}$. This method therefore ignores the population characteristics of the parents and offspring, instead providing only a snapshot of the population characteristics at a particular time.

It is therefore far more natural to use a distance in these studies. Whether reported in centiMorgans, base pairs, or some other measure, these measures remain constant over generations, and govern the dynamics of recombination.

\section{Conclusion} \label{sec:conclusion}

Despite this, many of the introductions to the field rely on the models of early pioneers of genetics. The works of Mendel, Pearson, Fisher, Haldane, and others in genetics were groundbreaking, but also occurred well before a modern understanding of DNA or the mechanics of inheritance \cite{visschergoddard2019}. As a result, these models do not provide a modern context. Modern textbooks and papers consequently introduce the structure of DNA and the models describing inheritance in separate sections, if the structure of DNA is addressed at all \cite{crowkimura1970intro, siegmundyakir2007, xu2013principles, liu1998statistical}. Such complete and detailed accounts with the biology and statistics separated are unquestionably important, but fail to present an accessible and unified picture of genomics for researchers with a statistical background. \TODO{Move to conclusion: ``this model provides a map to help understand larger works''}

\cite{laframboise2009} notes that microarrays remove the middle steps entirely: by taking luminance directly it is possible to inspect and relate $\ve{z}$ to physical characteristics without subjective intermediate steps.

This framework may not be limited to genomics. \cite{hasinetal2017multi} note the expansion of genome-wide methods to protein and RNA sequencing. In both of these cases, the framework above applies, but has less explanatory power. One might imagine investigating proteins using this framework to see which are relatively under or over-expressed

\bibliographystyle{plainnat}
\renewcommand*{\bibname}{References} % use title "References" for bibliography
\bibliography{../Bibliography/fullbib}

\end{document}
